{
 "metadata": {
  "name": "",
  "signature": "sha256:b395c8e9fbe54a72dd0e047c59fcf4363eef71e66138af889c773e1e09c31944"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Getting started with Python and the Natural Language Toolkit"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "user_nltk_dir = \"/home/researcher/lessons/nltk/nltk_data\"\n",
      "if user_nltk_dir not in nltk.data.path:\n",
      "    nltk.data.path.insert(0, user_nltk_dir)\n",
      "nltk.download(\"book\", download_dir=user_nltk_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data] Downloading collection u'book'\n",
        "[nltk_data]    | \n",
        "[nltk_data]    | Downloading package brown to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/brown.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package chat80 to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/chat80.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package cmudict to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/cmudict.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package conll2000 to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/conll2000.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package conll2002 to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/conll2002.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package dependency_treebank to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package genesis to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package gutenberg to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/gutenberg.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package ieer to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/ieer.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package inaugural to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/inaugural.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package movie_reviews to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/movie_reviews.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package nps_chat to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/nps_chat.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package names to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/names.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package ppattach to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/ppattach.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package reuters to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/reuters.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package senseval to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/senseval.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package state_union to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/state_union.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package stopwords to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/stopwords.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package swadesh to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/swadesh.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package timit to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/timit.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package treebank to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/treebank.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package toolbox to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/toolbox.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package udhr to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/udhr.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package udhr2 to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/udhr2.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package unicode_samples to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/unicode_samples.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package webtext to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/webtext.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package wordnet to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/wordnet.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package wordnet_ic to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package words to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping corpora/words.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package punkt to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping tokenizers/punkt.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package book_grammars to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    |   Unzipping grammars/book_grammars.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package city_database to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    |   Unzipping corpora/city_database.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]    | Downloading package tagsets to\n",
        "[nltk_data]    |     /home/researcher/lessons/nltk/nltk_data...\n",
        "[nltk_data]    | "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data]  Done downloading collection book\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "True"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.book import*"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exploring text - concordances, similar contexts, dispersion"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text1.concordance(\"monstrous\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "'Concordance'shows you a word in context. 'Similar' will find words used in similar contexts; remember it is not looking for synonyms, although the results may include synonyms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text1.similar(\"monstrous\")\n",
      "text2.similar(\"monstrous\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text2.common_contexts([\"monstrous\", \"very\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To represent information about a text graphically, import the Python library 'numpy'. We can generate a dispersion plot that shows where given words occur in a text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "%matplotlib inline\n",
      "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exploring vocabulary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "'len' gives the number of symbols or 'tokens' in your text. This is the total number of words and items of punctuation 'set' gives you a list of all the tokens in the text, without the duplicates. Hence len(set(text3)) will give you the total number unique tokens. Remember this still includes punctuation. 'sorted' places items in the list into alphabetical order, with punctuation symbols and capitalised words first."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(text3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(text3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(set(text3)) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can investigate the lexical richness of a text. For example, by dividing the total number of words by the number of words, we can see the average number of times each word is used. We can also count the number of times a word is used and calculate what percentage of the text it represents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(text3)/len(set(text3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4.count(\"American\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "100.0*text4.count(\"I\")/len(text4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Defining a function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You may wish to repeat an operation multiple times looking at different texts or different terms within a text. Instead of re-entering the formula every time, you can assign a name to the task, like \"lexical diversity\" and associate it with a block of code. This is called a Function. You can then re-use the function many times."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lexical_diversity(text):\n",
      "    return len(text)/len(set(text))\n",
      "lexical_diversity(text2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The parentheses are important here as they sepatate the the task, that is the work of the function, from the data that the function is to be performed on. The data in parentheses is called the argument of the function. When we use a function, we say that we 'call' it. Other functions that we've used already include len() and sorted() - these were predefined. Lexical_diversity() is one we set up ourselves; note that it's conventional to put a set of parentheses after a function, to make it clear what we're talking about."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Lists"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python treats a text as a long list of words. First, we'll make some lists of our own, to give you an idea of how a list behaves."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent1 = ['Call', 'me', 'Ishmael','.']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(sent1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The opening sentences of each of our texts have been pre-defined for you. You can inspect them by typing in 'sent2' etc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can add lists together, creating a new list containing all the items from both lists. You can do this by typing out the two lists or you can add two or more pre-defined lists. This is called concatenation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent4 + sent1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also add an item to the end of a list by appending. When we append(), the list itself is updated. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent1.append(\"Please\")\n",
      "sent1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Indexing Lists"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can navigate this list with the help of indexes. Just as we can find out the number of times a word occurs in a text, we can also find where a word first occurs. We can also navigate to different points in a text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4.index('awaken')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This works in reverse as well. We can ask Python to locate the 158th item in our list (note that we use square brackets here, not parentheses)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4.index[158]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As well as pulling out individual items from a list, indexes can be used to pull out selections of text from a large corpus to inspect. We call this slicing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text5[16715:16735]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we're asking for the beginning or end of a text, we can leave out the first or second number. For instance, [:5] will give us the first five items in a list while [8:] will give us all the elements from the eighth to the end. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text2[:10]\n",
      "text4[145700:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To help you understand how indexes work, let's create one. We start by defining the name of our index and then add the items. You probably won't do this in your own work, but you may want to manipulate an index in other ways. Pay attention to the quote marks and commas when you create your test sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "sent[0]\n",
      "sent[8]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the first element in the list is zero. This is because we are telling Python to go zero steps forward in the list. If we use an idnex that is too large (that is, we ask for something that doesn't exist), we'll get an error."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can modify elements in a list by assigning to one of its index values. We can also replace a slice with new material."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent[2]='furry'\n",
      "sent[7]='spotty'\n",
      "sent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent[4:5]=['gives', 'opera', 'tickets', 'to']\n",
      "sent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Defining variables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In Python, we give the items we're working with names, a process called assignment. For instance, in the NLTK corpus, 'Sense and Sensibility' has been assigned the name 'text2', which is much easier to work with. We also assigend the name 'sent' to the sentence that we created in the previous exercise, so that we could then instruct Python to do various things with it. Assigning a variable in python looks like this:\n",
      "variable = expression\n",
      "You can call your variables (almost) anything you like, but it's a good idea to pick names that will be meaningful and easy to type. You can't use words that already have a meaning in Python, such as import, def, or not. If you try to use a word that is reserved, you'll get a syntax error."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "opening= ['It', 'was', 'a', 'dark', 'and', 'stormy', 'night', ';' 'the', 'rain', 'fell', 'in', 'torrents']\n",
      "clause = opening[0:7]\n",
      "badger= sorted(clause)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that assigning a variable just causes Python to remember that information without generating any output. If you want Python to show you the result, you have to ask for it (this is a good thing when you assign a variable to a very long list!)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clause"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "badger"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exploring vocabulary 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use Python's ability to perform statistical analysis of data to do further exploration of vocabulary. For instance, we might want to be able to find the most common or least common words in a text. We'll start by looking at frequency distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist1=FreqDist(text1)\n",
      "print(fdist1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist1.most_common(50)\n",
      "fdist1('whale')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist1.plot(50,cumulative=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is possible to select the longest words in a text, which may tell you something about its vocabulary and style"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "V= set(text4)\n",
      "long_words = [word for word in V if len(word) > 15]\n",
      "sorted(long_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can fine-tune our selection even further by adding further conditions. For instance, we might want to find long words that occur frequently (or rarely)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist5 = FreqDist(text5)\n",
      "sorted (w for w in set(text5) if len(w)>7 and fdist5[w]>7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also find words that typically occur together, which tend to be very specific to a text or genre of texts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4.collocations()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As well as counting individual words, we can count other features of vocabulary, such as how often words of different lengths occur. We do this by putting together a number of the commands we've already learned.\n",
      "We could start like this: [len(w) for w in text1], but this would print the length of every word in the whole book, so let's skip that bit!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist= FreqDist(len(w) for w in text1)\n",
      "print(fdist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead, we can see how often each word length occurs, and sort by most common word length. We can use other commands in Python to explore our data further"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist.most_common()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist.max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist.freq(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These last two commands tell us that the most common word length is 3, and that these 3 letter words account for about 20% of the book. We can see this just by visually inspecting the list produced by fdist.most_common(), but if this list were too long to inspect readily, or we didn't want to print it, there are others ways to explore it.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist[9]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a number of functions defined for NLTK's frequency distributions\n",
      "\n",
      "fdist = FreqDist(samples) \tcreate a frequency distribution containing the given samples\n",
      "fdist[sample] += 1 \tincrement the count for this sample\n",
      "fdist['monstrous'] \tcount of the number of times a given sample occurred\n",
      "fdist.freq('monstrous') \tfrequency of a given sample\n",
      "fdist.N() \ttotal number of samples\n",
      "fdist.most_common(n) \tthe n most common samples and their frequencies\n",
      "for sample in fdist: \titerate over the samples\n",
      "fdist.max() \tsample with the greatest count\n",
      "fdist.tabulate() \ttabulate the frequency distribution\n",
      "fdist.plot() \tgraphical plot of the frequency distribution\n",
      "fdist.plot(cumulative=True) \tcumulative plot of the frequency distribution\n",
      "fdist1 |= fdist2 \tupdate fdist1 with counts from fdist2\n",
      "fdist1 < fdist2 \ttest if samples in fdist1 occur less frequently than in fdist2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist=FreqDist(text1)\n",
      "fdist['monstrous']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also use numerical operators to refine the types of searches we ask Python to run. We can use the following relational operators:\n",
      "< \tless than\n",
      "<= \tless than or equal to\n",
      "== \tequal to (note this is two \"=\" signs, not one)\n",
      "!= \tnot equal to\n",
      "> \tgreater than\n",
      ">= \tgreater than or equal to"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenge: using one of the pre-defined sentences in the NLTK corpus, use the relational operators above to find:\n",
      "Words longer than four characters\n",
      "Words of four or more characters\n",
      "Words of exactly four characters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also look for features such as letter combinations, upper and lowercase letters, and digits. some operators you might like to use are:\n",
      "s.startswith(t) test if s starts with t\n",
      "s.endswith(t) \ttest if s ends with t\n",
      "t in s \t        test if t is a substring of s\n",
      "s.islower() \ttest if s contains cased characters and all are lowercase\n",
      "s.isupper() \ttest if s contains cased characters and all are uppercase\n",
      "s.isalpha() \ttest if s is non-empty and all characters in s are alphabetic\n",
      "s.isalnum() \ttest if s is non-empty and all characters in s are alphanumeric\n",
      "s.isdigit() \ttest if s is non-empty and all characters in s are digits\n",
      "s.istitle() \ttest if s contains cased characters and is titlecased (i.e. all words in s have initial capitals)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted (w for w in set(text1) if w.endswith('ableness'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(n for n in sent7 if n.isdigit())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bonus! \n",
      "You'll remember right at the beginning we started looking at the size of the vocabulary of a text, but there were two problems with the results we got from using len(set(text1). This count includes items of punctuation and treats capitalised and non-capitalised words as different things ('This' vs 'this'). We can now fix this. We can start by getting rid of capitalised words, then we can get rid of the punctuation and numbers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(word.lower() for word in text1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(word.lower() for word in text1 if word.isalpha()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}