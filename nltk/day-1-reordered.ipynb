{
 "metadata": {
  "signature": "sha256:6f90598c265f9ad38e5f2a75064bd7dbf289918d743b19f762493622464ae834"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Session 1: Orientation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<img style=\"float:left\" src=\"http://ipython.org/_static/IPy_header.png\" />\n",
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "Welcome to the *IPython Notebook*. Through this interface, you'll be learning a lot of things:\n",
      "\n",
      "* A Programming language: **Python**\n",
      "* A Python library: **NLTK**\n",
      "* Overlapping research areas: **Corpus linguistics**, **Natural language processing**, **Distant reading**\n",
      "* Additional skills: **Regular Expressions**, some **Shell commands**, and **tips on managing your data**\n",
      "\n",
      "You can head [here](https://github.com/interrogator/lessons/blob/master/nltk/session-plan.md) for the fully articulated overview of the course, but we'll almost always stay within IPython. Remember, everything we cover here will remain available to you after ResBaz is over, including these Notebooks. It's all accessible at the [ResBaz Github](https://github.com/resbaz).\n",
      "\n",
      "**Any questions before we begin?**\n",
      "\n",
      "Alright, we're off!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What is the Natural Language Toolkit?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "We'll be covering some of the theory behind corpus linguistics later on, but let's start by looking at some of the tasks NLTK can help you with. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK is a Python Library for working with written language data. It is free and extensively documented. Many areas we'll be covering are treated in more detail in the NLTK Book, available free online from [here](http://www.nltk.org/book/).\n",
      "\n",
      "> Note: NLTK provides tools for tasks ranging from very simple (counting words in a text) to very complex (writing and training parsers, etc.). Many advanced tasks are beyond the scope of this course, but by the time we're done, you should understand Python and NLTK well enough to perform these tasks on your own!\n",
      "\n",
      "We will start by importing NLTK, setting a path to NLTK resources, and downloading some additional stuff."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk # imports all the nltk basics\n",
      "user_nltk_dir = \"/home/researcher/lessons/nltk/nltk_data\" # specify our data directory\n",
      "if user_nltk_dir not in nltk.data.path: # make sure nltk can access this dir\n",
      "    nltk.data.path.insert(0, user_nltk_dir)\n",
      "nltk.download(\"book\", download_dir=user_nltk_dir) # download book materials to data directory"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Oh, we've got to import some corpora used in the book as well..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.book import *  # asterisk means 'everything'\n",
      "\n",
      "# Importing the book has assigned variable names to ten corpora. We can call these names easily: "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "*** Introductory Examples for the NLTK Book ***\n",
        "Loading text1, ..., text9 and sent1, ..., sent9\n",
        "Type the name of the text or sentence to view it.\n",
        "Type: 'texts()' or 'sents()' to list the materials.\n",
        "text1:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Moby Dick by Herman Melville 1851\n",
        "text2:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Sense and Sensibility by Jane Austen 1811\n",
        "text3:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " The Book of Genesis\n",
        "text4:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Inaugural Address Corpus\n",
        "text5:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Chat Corpus\n",
        "text6:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Monty Python and the Holy Grail\n",
        "text7:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Wall Street Journal\n",
        "text8: Personals Corpus\n",
        "text9:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " The Man Who Was Thursday by G . K . Chesterton 1908\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#text1\n",
      "text2\n",
      "#text3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "<Text: Sense and Sensibility by Jane Austen 1811>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exploring vocabulary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK makes it really easy to get basic information about the size of a text and the complexity of its vocalbulary\n",
      "*len* gives the number of symbols or 'tokens' in your text. This is the total number of words and items of punctuation.\n",
      "\n",
      "*set* gives you a list of all the tokens in the text, without the duplicates.\n",
      "\n",
      "Hence, **len(set(text3))** will give you the total number unique tokens. Remember this still includes punctuation. 'sorted' places items in the list into alphabetical order, with punctuation symbols and capitalised words first."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(text3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "44764"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(text3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "2789"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(set(text3)) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[u'!',\n",
        " u\"'\",\n",
        " u'(',\n",
        " u')',\n",
        " u',',\n",
        " u',)',\n",
        " u'.',\n",
        " u'.)',\n",
        " u':',\n",
        " u';',\n",
        " u';)',\n",
        " u'?',\n",
        " u'?)',\n",
        " u'A',\n",
        " u'Abel',\n",
        " u'Abelmizraim',\n",
        " u'Abidah',\n",
        " u'Abide',\n",
        " u'Abimael',\n",
        " u'Abimelech',\n",
        " u'Abr',\n",
        " u'Abrah',\n",
        " u'Abraham',\n",
        " u'Abram',\n",
        " u'Accad',\n",
        " u'Achbor',\n",
        " u'Adah',\n",
        " u'Adam',\n",
        " u'Adbeel',\n",
        " u'Admah',\n",
        " u'Adullamite',\n",
        " u'After',\n",
        " u'Aholibamah',\n",
        " u'Ahuzzath',\n",
        " u'Ajah',\n",
        " u'Akan',\n",
        " u'All',\n",
        " u'Allonbachuth',\n",
        " u'Almighty',\n",
        " u'Almodad',\n",
        " u'Also',\n",
        " u'Alvah',\n",
        " u'Alvan',\n",
        " u'Am',\n",
        " u'Amal',\n",
        " u'Amalek',\n",
        " u'Amalekites',\n",
        " u'Ammon',\n",
        " u'Amorite',\n",
        " u'Amorites',\n",
        " u'Amraphel',\n",
        " u'An',\n",
        " u'Anah',\n",
        " u'Anamim',\n",
        " u'And',\n",
        " u'Aner',\n",
        " u'Angel',\n",
        " u'Appoint',\n",
        " u'Aram',\n",
        " u'Aran',\n",
        " u'Ararat',\n",
        " u'Arbah',\n",
        " u'Ard',\n",
        " u'Are',\n",
        " u'Areli',\n",
        " u'Arioch',\n",
        " u'Arise',\n",
        " u'Arkite',\n",
        " u'Arodi',\n",
        " u'Arphaxad',\n",
        " u'Art',\n",
        " u'Arvadite',\n",
        " u'As',\n",
        " u'Asenath',\n",
        " u'Ashbel',\n",
        " u'Asher',\n",
        " u'Ashkenaz',\n",
        " u'Ashteroth',\n",
        " u'Ask',\n",
        " u'Asshur',\n",
        " u'Asshurim',\n",
        " u'Assyr',\n",
        " u'Assyria',\n",
        " u'At',\n",
        " u'Atad',\n",
        " u'Avith',\n",
        " u'Baalhanan',\n",
        " u'Babel',\n",
        " u'Bashemath',\n",
        " u'Be',\n",
        " u'Because',\n",
        " u'Becher',\n",
        " u'Bedad',\n",
        " u'Beeri',\n",
        " u'Beerlahairoi',\n",
        " u'Beersheba',\n",
        " u'Behold',\n",
        " u'Bela',\n",
        " u'Belah',\n",
        " u'Benam',\n",
        " u'Benjamin',\n",
        " u'Beno',\n",
        " u'Beor',\n",
        " u'Bera',\n",
        " u'Bered',\n",
        " u'Beriah',\n",
        " u'Bethel',\n",
        " u'Bethlehem',\n",
        " u'Bethuel',\n",
        " u'Beware',\n",
        " u'Bilhah',\n",
        " u'Bilhan',\n",
        " u'Binding',\n",
        " u'Birsha',\n",
        " u'Bless',\n",
        " u'Blessed',\n",
        " u'Both',\n",
        " u'Bow',\n",
        " u'Bozrah',\n",
        " u'Bring',\n",
        " u'But',\n",
        " u'Buz',\n",
        " u'By',\n",
        " u'Cain',\n",
        " u'Cainan',\n",
        " u'Calah',\n",
        " u'Calneh',\n",
        " u'Can',\n",
        " u'Cana',\n",
        " u'Canaan',\n",
        " u'Canaanite',\n",
        " u'Canaanites',\n",
        " u'Canaanitish',\n",
        " u'Caphtorim',\n",
        " u'Carmi',\n",
        " u'Casluhim',\n",
        " u'Cast',\n",
        " u'Cause',\n",
        " u'Chaldees',\n",
        " u'Chedorlaomer',\n",
        " u'Cheran',\n",
        " u'Cherubims',\n",
        " u'Chesed',\n",
        " u'Chezib',\n",
        " u'Come',\n",
        " u'Cursed',\n",
        " u'Cush',\n",
        " u'Damascus',\n",
        " u'Dan',\n",
        " u'Day',\n",
        " u'Deborah',\n",
        " u'Dedan',\n",
        " u'Deliver',\n",
        " u'Diklah',\n",
        " u'Din',\n",
        " u'Dinah',\n",
        " u'Dinhabah',\n",
        " u'Discern',\n",
        " u'Dishan',\n",
        " u'Dishon',\n",
        " u'Do',\n",
        " u'Dodanim',\n",
        " u'Dothan',\n",
        " u'Drink',\n",
        " u'Duke',\n",
        " u'Dumah',\n",
        " u'Earth',\n",
        " u'Ebal',\n",
        " u'Eber',\n",
        " u'Edar',\n",
        " u'Eden',\n",
        " u'Edom',\n",
        " u'Edomites',\n",
        " u'Egy',\n",
        " u'Egypt',\n",
        " u'Egyptia',\n",
        " u'Egyptian',\n",
        " u'Egyptians',\n",
        " u'Ehi',\n",
        " u'Elah',\n",
        " u'Elam',\n",
        " u'Elbethel',\n",
        " u'Eldaah',\n",
        " u'EleloheIsrael',\n",
        " u'Eliezer',\n",
        " u'Eliphaz',\n",
        " u'Elishah',\n",
        " u'Ellasar',\n",
        " u'Elon',\n",
        " u'Elparan',\n",
        " u'Emins',\n",
        " u'En',\n",
        " u'Enmishpat',\n",
        " u'Eno',\n",
        " u'Enoch',\n",
        " u'Enos',\n",
        " u'Ephah',\n",
        " u'Epher',\n",
        " u'Ephra',\n",
        " u'Ephraim',\n",
        " u'Ephrath',\n",
        " u'Ephron',\n",
        " u'Er',\n",
        " u'Erech',\n",
        " u'Eri',\n",
        " u'Es',\n",
        " u'Esau',\n",
        " u'Escape',\n",
        " u'Esek',\n",
        " u'Eshban',\n",
        " u'Eshcol',\n",
        " u'Ethiopia',\n",
        " u'Euphrat',\n",
        " u'Euphrates',\n",
        " u'Eve',\n",
        " u'Even',\n",
        " u'Every',\n",
        " u'Except',\n",
        " u'Ezbon',\n",
        " u'Ezer',\n",
        " u'Fear',\n",
        " u'Feed',\n",
        " u'Fifteen',\n",
        " u'Fill',\n",
        " u'For',\n",
        " u'Forasmuch',\n",
        " u'Forgive',\n",
        " u'From',\n",
        " u'Fulfil',\n",
        " u'G',\n",
        " u'Gad',\n",
        " u'Gaham',\n",
        " u'Galeed',\n",
        " u'Gatam',\n",
        " u'Gather',\n",
        " u'Gaza',\n",
        " u'Gentiles',\n",
        " u'Gera',\n",
        " u'Gerar',\n",
        " u'Gershon',\n",
        " u'Get',\n",
        " u'Gether',\n",
        " u'Gihon',\n",
        " u'Gilead',\n",
        " u'Girgashites',\n",
        " u'Girgasite',\n",
        " u'Give',\n",
        " u'Go',\n",
        " u'God',\n",
        " u'Gomer',\n",
        " u'Gomorrah',\n",
        " u'Goshen',\n",
        " u'Guni',\n",
        " u'Hadad',\n",
        " u'Hadar',\n",
        " u'Hadoram',\n",
        " u'Hagar',\n",
        " u'Haggi',\n",
        " u'Hai',\n",
        " u'Ham',\n",
        " u'Hamathite',\n",
        " u'Hamor',\n",
        " u'Hamul',\n",
        " u'Hanoch',\n",
        " u'Happy',\n",
        " u'Haran',\n",
        " u'Hast',\n",
        " u'Haste',\n",
        " u'Have',\n",
        " u'Havilah',\n",
        " u'Hazarmaveth',\n",
        " u'Hazezontamar',\n",
        " u'Hazo',\n",
        " u'He',\n",
        " u'Hear',\n",
        " u'Heaven',\n",
        " u'Heber',\n",
        " u'Hebrew',\n",
        " u'Hebrews',\n",
        " u'Hebron',\n",
        " u'Hemam',\n",
        " u'Hemdan',\n",
        " u'Here',\n",
        " u'Hereby',\n",
        " u'Heth',\n",
        " u'Hezron',\n",
        " u'Hiddekel',\n",
        " u'Hinder',\n",
        " u'Hirah',\n",
        " u'His',\n",
        " u'Hitti',\n",
        " u'Hittite',\n",
        " u'Hittites',\n",
        " u'Hivite',\n",
        " u'Hobah',\n",
        " u'Hori',\n",
        " u'Horite',\n",
        " u'Horites',\n",
        " u'How',\n",
        " u'Hul',\n",
        " u'Huppim',\n",
        " u'Husham',\n",
        " u'Hushim',\n",
        " u'Huz',\n",
        " u'I',\n",
        " u'If',\n",
        " u'In',\n",
        " u'Irad',\n",
        " u'Iram',\n",
        " u'Is',\n",
        " u'Isa',\n",
        " u'Isaac',\n",
        " u'Iscah',\n",
        " u'Ishbak',\n",
        " u'Ishmael',\n",
        " u'Ishmeelites',\n",
        " u'Ishuah',\n",
        " u'Isra',\n",
        " u'Israel',\n",
        " u'Issachar',\n",
        " u'Isui',\n",
        " u'It',\n",
        " u'Ithran',\n",
        " u'Jaalam',\n",
        " u'Jabal',\n",
        " u'Jabbok',\n",
        " u'Jac',\n",
        " u'Jachin',\n",
        " u'Jacob',\n",
        " u'Jahleel',\n",
        " u'Jahzeel',\n",
        " u'Jamin',\n",
        " u'Japhe',\n",
        " u'Japheth',\n",
        " u'Jared',\n",
        " u'Javan',\n",
        " u'Jebusite',\n",
        " u'Jebusites',\n",
        " u'Jegarsahadutha',\n",
        " u'Jehovahjireh',\n",
        " u'Jemuel',\n",
        " u'Jerah',\n",
        " u'Jetheth',\n",
        " u'Jetur',\n",
        " u'Jeush',\n",
        " u'Jezer',\n",
        " u'Jidlaph',\n",
        " u'Jimnah',\n",
        " u'Job',\n",
        " u'Jobab',\n",
        " u'Jokshan',\n",
        " u'Joktan',\n",
        " u'Jordan',\n",
        " u'Joseph',\n",
        " u'Jubal',\n",
        " u'Judah',\n",
        " u'Judge',\n",
        " u'Judith',\n",
        " u'Kadesh',\n",
        " u'Kadmonites',\n",
        " u'Karnaim',\n",
        " u'Kedar',\n",
        " u'Kedemah',\n",
        " u'Kemuel',\n",
        " u'Kenaz',\n",
        " u'Kenites',\n",
        " u'Kenizzites',\n",
        " u'Keturah',\n",
        " u'Kiriathaim',\n",
        " u'Kirjatharba',\n",
        " u'Kittim',\n",
        " u'Know',\n",
        " u'Kohath',\n",
        " u'Kor',\n",
        " u'Korah',\n",
        " u'LO',\n",
        " u'LORD',\n",
        " u'Laban',\n",
        " u'Lahairoi',\n",
        " u'Lamech',\n",
        " u'Lasha',\n",
        " u'Lay',\n",
        " u'Leah',\n",
        " u'Lehabim',\n",
        " u'Lest',\n",
        " u'Let',\n",
        " u'Letushim',\n",
        " u'Leummim',\n",
        " u'Levi',\n",
        " u'Lie',\n",
        " u'Lift',\n",
        " u'Lo',\n",
        " u'Look',\n",
        " u'Lot',\n",
        " u'Lotan',\n",
        " u'Lud',\n",
        " u'Ludim',\n",
        " u'Luz',\n",
        " u'Maachah',\n",
        " u'Machir',\n",
        " u'Machpelah',\n",
        " u'Madai',\n",
        " u'Magdiel',\n",
        " u'Magog',\n",
        " u'Mahalaleel',\n",
        " u'Mahalath',\n",
        " u'Mahanaim',\n",
        " u'Make',\n",
        " u'Malchiel',\n",
        " u'Male',\n",
        " u'Mam',\n",
        " u'Mamre',\n",
        " u'Man',\n",
        " u'Manahath',\n",
        " u'Manass',\n",
        " u'Manasseh',\n",
        " u'Mash',\n",
        " u'Masrekah',\n",
        " u'Massa',\n",
        " u'Matred',\n",
        " u'Me',\n",
        " u'Medan',\n",
        " u'Mehetabel',\n",
        " u'Mehujael',\n",
        " u'Melchizedek',\n",
        " u'Merari',\n",
        " u'Mesha',\n",
        " u'Meshech',\n",
        " u'Mesopotamia',\n",
        " u'Methusa',\n",
        " u'Methusael',\n",
        " u'Methuselah',\n",
        " u'Mezahab',\n",
        " u'Mibsam',\n",
        " u'Mibzar',\n",
        " u'Midian',\n",
        " u'Midianites',\n",
        " u'Milcah',\n",
        " u'Mishma',\n",
        " u'Mizpah',\n",
        " u'Mizraim',\n",
        " u'Mizz',\n",
        " u'Moab',\n",
        " u'Moabites',\n",
        " u'Moreh',\n",
        " u'Moreover',\n",
        " u'Moriah',\n",
        " u'Muppim',\n",
        " u'My',\n",
        " u'Naamah',\n",
        " u'Naaman',\n",
        " u'Nahath',\n",
        " u'Nahor',\n",
        " u'Naphish',\n",
        " u'Naphtali',\n",
        " u'Naphtuhim',\n",
        " u'Nay',\n",
        " u'Nebajoth',\n",
        " u'Neither',\n",
        " u'Night',\n",
        " u'Nimrod',\n",
        " u'Nineveh',\n",
        " u'Noah',\n",
        " u'Nod',\n",
        " u'Not',\n",
        " u'Now',\n",
        " u'O',\n",
        " u'Obal',\n",
        " u'Of',\n",
        " u'Oh',\n",
        " u'Ohad',\n",
        " u'Omar',\n",
        " u'On',\n",
        " u'Onam',\n",
        " u'Onan',\n",
        " u'Only',\n",
        " u'Ophir',\n",
        " u'Our',\n",
        " u'Out',\n",
        " u'Padan',\n",
        " u'Padanaram',\n",
        " u'Paran',\n",
        " u'Pass',\n",
        " u'Pathrusim',\n",
        " u'Pau',\n",
        " u'Peace',\n",
        " u'Peleg',\n",
        " u'Peniel',\n",
        " u'Penuel',\n",
        " u'Peradventure',\n",
        " u'Perizzit',\n",
        " u'Perizzite',\n",
        " u'Perizzites',\n",
        " u'Phallu',\n",
        " u'Phara',\n",
        " u'Pharaoh',\n",
        " u'Pharez',\n",
        " u'Phichol',\n",
        " u'Philistim',\n",
        " u'Philistines',\n",
        " u'Phut',\n",
        " u'Phuvah',\n",
        " u'Pildash',\n",
        " u'Pinon',\n",
        " u'Pison',\n",
        " u'Potiphar',\n",
        " u'Potipherah',\n",
        " u'Put',\n",
        " u'Raamah',\n",
        " u'Rachel',\n",
        " u'Rameses',\n",
        " u'Rebek',\n",
        " u'Rebekah',\n",
        " u'Rehoboth',\n",
        " u'Remain',\n",
        " u'Rephaims',\n",
        " u'Resen',\n",
        " u'Return',\n",
        " u'Reu',\n",
        " u'Reub',\n",
        " u'Reuben',\n",
        " u'Reuel',\n",
        " u'Reumah',\n",
        " u'Riphath',\n",
        " u'Rosh',\n",
        " u'Sabtah',\n",
        " u'Sabtech',\n",
        " u'Said',\n",
        " u'Salah',\n",
        " u'Salem',\n",
        " u'Samlah',\n",
        " u'Sarah',\n",
        " u'Sarai',\n",
        " u'Saul',\n",
        " u'Save',\n",
        " u'Say',\n",
        " u'Se',\n",
        " u'Seba',\n",
        " u'See',\n",
        " u'Seeing',\n",
        " u'Seir',\n",
        " u'Sell',\n",
        " u'Send',\n",
        " u'Sephar',\n",
        " u'Serah',\n",
        " u'Sered',\n",
        " u'Serug',\n",
        " u'Set',\n",
        " u'Seth',\n",
        " u'Shalem',\n",
        " u'Shall',\n",
        " u'Shalt',\n",
        " u'Shammah',\n",
        " u'Shaul',\n",
        " u'Shaveh',\n",
        " u'She',\n",
        " u'Sheba',\n",
        " u'Shebah',\n",
        " u'Shechem',\n",
        " u'Shed',\n",
        " u'Shel',\n",
        " u'Shelah',\n",
        " u'Sheleph',\n",
        " u'Shem',\n",
        " u'Shemeber',\n",
        " u'Shepho',\n",
        " u'Shillem',\n",
        " u'Shiloh',\n",
        " u'Shimron',\n",
        " u'Shinab',\n",
        " u'Shinar',\n",
        " u'Shobal',\n",
        " u'Should',\n",
        " u'Shuah',\n",
        " u'Shuni',\n",
        " u'Shur',\n",
        " u'Sichem',\n",
        " u'Siddim',\n",
        " u'Sidon',\n",
        " u'Simeon',\n",
        " u'Sinite',\n",
        " u'Sitnah',\n",
        " u'Slay',\n",
        " u'So',\n",
        " u'Sod',\n",
        " u'Sodom',\n",
        " u'Sojourn',\n",
        " u'Some',\n",
        " u'Spake',\n",
        " u'Speak',\n",
        " u'Spirit',\n",
        " u'Stand',\n",
        " u'Succoth',\n",
        " u'Surely',\n",
        " u'Swear',\n",
        " u'Syrian',\n",
        " u'Take',\n",
        " u'Tamar',\n",
        " u'Tarshish',\n",
        " u'Tebah',\n",
        " u'Tell',\n",
        " u'Tema',\n",
        " u'Teman',\n",
        " u'Temani',\n",
        " u'Terah',\n",
        " u'Thahash',\n",
        " u'That',\n",
        " u'The',\n",
        " u'Then',\n",
        " u'There',\n",
        " u'Therefore',\n",
        " u'These',\n",
        " u'They',\n",
        " u'Thirty',\n",
        " u'This',\n",
        " u'Thorns',\n",
        " u'Thou',\n",
        " u'Thus',\n",
        " u'Thy',\n",
        " u'Tidal',\n",
        " u'Timna',\n",
        " u'Timnah',\n",
        " u'Timnath',\n",
        " u'Tiras',\n",
        " u'To',\n",
        " u'Togarmah',\n",
        " u'Tola',\n",
        " u'Tubal',\n",
        " u'Tubalcain',\n",
        " u'Twelve',\n",
        " u'Two',\n",
        " u'Unstable',\n",
        " u'Until',\n",
        " u'Unto',\n",
        " u'Up',\n",
        " u'Upon',\n",
        " u'Ur',\n",
        " u'Uz',\n",
        " u'Uzal',\n",
        " u'We',\n",
        " u'What',\n",
        " u'When',\n",
        " u'Whence',\n",
        " u'Where',\n",
        " u'Whereas',\n",
        " u'Wherefore',\n",
        " u'Which',\n",
        " u'While',\n",
        " u'Who',\n",
        " u'Whose',\n",
        " u'Whoso',\n",
        " u'Why',\n",
        " u'Wilt',\n",
        " u'With',\n",
        " u'Woman',\n",
        " u'Ye',\n",
        " u'Yea',\n",
        " u'Yet',\n",
        " u'Zaavan',\n",
        " u'Zaphnathpaaneah',\n",
        " u'Zar',\n",
        " u'Zarah',\n",
        " u'Zeboiim',\n",
        " u'Zeboim',\n",
        " u'Zebul',\n",
        " u'Zebulun',\n",
        " u'Zemarite',\n",
        " u'Zepho',\n",
        " u'Zerah',\n",
        " u'Zibeon',\n",
        " u'Zidon',\n",
        " u'Zillah',\n",
        " u'Zilpah',\n",
        " u'Zimran',\n",
        " u'Ziphion',\n",
        " u'Zo',\n",
        " u'Zoar',\n",
        " u'Zohar',\n",
        " u'Zuzims',\n",
        " u'a',\n",
        " u'abated',\n",
        " u'abide',\n",
        " u'able',\n",
        " u'abode',\n",
        " u'abomination',\n",
        " u'about',\n",
        " u'above',\n",
        " u'abroad',\n",
        " u'absent',\n",
        " u'abundantly',\n",
        " u'accept',\n",
        " u'accepted',\n",
        " u'according',\n",
        " u'acknowledged',\n",
        " u'activity',\n",
        " u'add',\n",
        " u'adder',\n",
        " u'afar',\n",
        " u'afflict',\n",
        " u'affliction',\n",
        " u'afraid',\n",
        " u'after',\n",
        " u'afterward',\n",
        " u'afterwards',\n",
        " u'aga',\n",
        " u'again',\n",
        " u'against',\n",
        " u'age',\n",
        " u'aileth',\n",
        " u'air',\n",
        " u'al',\n",
        " u'alive',\n",
        " u'all',\n",
        " u'almon',\n",
        " u'alo',\n",
        " u'alone',\n",
        " u'aloud',\n",
        " u'also',\n",
        " u'altar',\n",
        " u'altogether',\n",
        " u'always',\n",
        " u'am',\n",
        " u'among',\n",
        " u'amongst',\n",
        " u'an',\n",
        " u'and',\n",
        " u'angel',\n",
        " u'angels',\n",
        " u'anger',\n",
        " u'angry',\n",
        " u'anguish',\n",
        " u'anointedst',\n",
        " u'anoth',\n",
        " u'another',\n",
        " u'answer',\n",
        " u'answered',\n",
        " u'any',\n",
        " u'anything',\n",
        " u'appe',\n",
        " u'appear',\n",
        " u'appeared',\n",
        " u'appease',\n",
        " u'appoint',\n",
        " u'appointed',\n",
        " u'aprons',\n",
        " u'archer',\n",
        " u'archers',\n",
        " u'are',\n",
        " u'arise',\n",
        " u'ark',\n",
        " u'armed',\n",
        " u'arms',\n",
        " u'army',\n",
        " u'arose',\n",
        " u'arrayed',\n",
        " u'art',\n",
        " u'artificer',\n",
        " u'as',\n",
        " u'ascending',\n",
        " u'ash',\n",
        " u'ashamed',\n",
        " u'ask',\n",
        " u'asked',\n",
        " u'asketh',\n",
        " u'ass',\n",
        " u'assembly',\n",
        " u'asses',\n",
        " u'assigned',\n",
        " u'asswaged',\n",
        " u'at',\n",
        " u'attained',\n",
        " u'audience',\n",
        " u'avenged',\n",
        " u'aw',\n",
        " u'awaked',\n",
        " u'away',\n",
        " u'awoke',\n",
        " u'back',\n",
        " u'backward',\n",
        " u'bad',\n",
        " u'bade',\n",
        " u'badest',\n",
        " u'badne',\n",
        " u'bak',\n",
        " u'bake',\n",
        " u'bakemeats',\n",
        " u'baker',\n",
        " u'bakers',\n",
        " u'balm',\n",
        " u'bands',\n",
        " u'bank',\n",
        " u'bare',\n",
        " u'barr',\n",
        " u'barren',\n",
        " u'basket',\n",
        " u'baskets',\n",
        " u'battle',\n",
        " u'bdellium',\n",
        " u'be',\n",
        " u'bear',\n",
        " u'beari',\n",
        " u'bearing',\n",
        " u'beast',\n",
        " u'beasts',\n",
        " u'beautiful',\n",
        " u'became',\n",
        " u'because',\n",
        " u'become',\n",
        " u'bed',\n",
        " u'been',\n",
        " u'befall',\n",
        " u'befell',\n",
        " u'before',\n",
        " u'began',\n",
        " u'begat',\n",
        " u'beget',\n",
        " u'begettest',\n",
        " u'begin',\n",
        " u'beginning',\n",
        " u'begotten',\n",
        " u'beguiled',\n",
        " u'beheld',\n",
        " u'behind',\n",
        " u'behold',\n",
        " u'being',\n",
        " u'believed',\n",
        " u'belly',\n",
        " u'belong',\n",
        " u'beneath',\n",
        " u'bereaved',\n",
        " u'beside',\n",
        " u'besides',\n",
        " u'besought',\n",
        " u'best',\n",
        " u'betimes',\n",
        " u'better',\n",
        " u'between',\n",
        " u'betwixt',\n",
        " u'beyond',\n",
        " u'binding',\n",
        " u'bird',\n",
        " u'birds',\n",
        " u'birthday',\n",
        " u'birthright',\n",
        " u'biteth',\n",
        " u'bitter',\n",
        " u'blame',\n",
        " u'blameless',\n",
        " u'blasted',\n",
        " u'bless',\n",
        " u'blessed',\n",
        " u'blesseth',\n",
        " u'blessi',\n",
        " u'blessing',\n",
        " u'blessings',\n",
        " u'blindness',\n",
        " u'blood',\n",
        " u'blossoms',\n",
        " u'bodies',\n",
        " u'boldly',\n",
        " u'bondman',\n",
        " u'bondmen',\n",
        " u'bondwoman',\n",
        " u'bone',\n",
        " u'bones',\n",
        " u'book',\n",
        " u'booths',\n",
        " u'border',\n",
        " u'borders',\n",
        " u'born',\n",
        " u'bosom',\n",
        " u'both',\n",
        " u'bottle',\n",
        " u'bou',\n",
        " u'boug',\n",
        " u'bough',\n",
        " u'bought',\n",
        " u'bound',\n",
        " u'bow',\n",
        " u'bowed',\n",
        " u'bowels',\n",
        " u'bowing',\n",
        " u'boys',\n",
        " u'bracelets',\n",
        " u'branches',\n",
        " u'brass',\n",
        " u'bre',\n",
        " u'breach',\n",
        " u'bread',\n",
        " u'breadth',\n",
        " u'break',\n",
        " u'breaketh',\n",
        " u'breaking',\n",
        " u'breasts',\n",
        " u'breath',\n",
        " u'breathed',\n",
        " u'breed',\n",
        " u'brethren',\n",
        " u'brick',\n",
        " u'brimstone',\n",
        " u'bring',\n",
        " u'brink',\n",
        " u'broken',\n",
        " u'brook',\n",
        " u'broth',\n",
        " u'brother',\n",
        " u'brought',\n",
        " u'brown',\n",
        " u'bruise',\n",
        " u'budded',\n",
        " u'build',\n",
        " u'builded',\n",
        " u'built',\n",
        " u'bulls',\n",
        " u'bundle',\n",
        " u'bundles',\n",
        " u'burdens',\n",
        " u'buried',\n",
        " u'burn',\n",
        " u'burning',\n",
        " u'burnt',\n",
        " u'bury',\n",
        " u'buryingplace',\n",
        " u'business',\n",
        " u'but',\n",
        " u'butler',\n",
        " u'butlers',\n",
        " u'butlership',\n",
        " u'butter',\n",
        " u'buy',\n",
        " u'by',\n",
        " u'cakes',\n",
        " u'calf',\n",
        " u'call',\n",
        " u'called',\n",
        " u'came',\n",
        " u'camel',\n",
        " u'camels',\n",
        " u'camest',\n",
        " u'can',\n",
        " u'cannot',\n",
        " u'canst',\n",
        " u'captain',\n",
        " u'captive',\n",
        " u'captives',\n",
        " u'carcases',\n",
        " u'carried',\n",
        " u'carry',\n",
        " u'cast',\n",
        " u'castles',\n",
        " u'catt',\n",
        " u'cattle',\n",
        " u'caught',\n",
        " u'cause',\n",
        " u'caused',\n",
        " u'cave',\n",
        " u'cease',\n",
        " u'ceased',\n",
        " u'certain',\n",
        " u'certainly',\n",
        " u'chain',\n",
        " u'chamber',\n",
        " u'change',\n",
        " u'changed',\n",
        " u'changes',\n",
        " u'charge',\n",
        " u'charged',\n",
        " u'chariot',\n",
        " u'chariots',\n",
        " u'chesnut',\n",
        " u'chi',\n",
        " u'chief',\n",
        " u'child',\n",
        " u'childless',\n",
        " u'childr',\n",
        " u'children',\n",
        " u'chode',\n",
        " u'choice',\n",
        " u'chose',\n",
        " u'circumcis',\n",
        " u'circumcise',\n",
        " u'circumcised',\n",
        " u'citi',\n",
        " u'cities',\n",
        " u'city',\n",
        " u'clave',\n",
        " u'clean',\n",
        " u'clear',\n",
        " u'cleave',\n",
        " u'clo',\n",
        " u'closed',\n",
        " u'clothed',\n",
        " u'clothes',\n",
        " u'cloud',\n",
        " u'clusters',\n",
        " u'co',\n",
        " u'coat',\n",
        " u'coats',\n",
        " u'coffin',\n",
        " u'cold',\n",
        " ...]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can investigate the *lexical richness* of a text. For example, by dividing the total number of words by the number of words, we can see the average number of times each word is used. We can also count the number of times a word is used and calculate what percentage of the text it represents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(text3)/len(set(text3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "16"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4.count(\"American\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "147"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Challenge! How would you calculate the percentage of Text 4 that is taken up by the word \"America\"?\n",
      "100.0*text4.count(\"America\")/len(text4)# "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "0.13174597728754245"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exploring text - concordances, similar contexts, dispersion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "'Concordance' shows you a word in context and is useful if you want to be able to discuss the ways in which a word is used in a text \n",
      "Similar' will find words used in similar contexts; remember it is not looking for synonyms, \n",
      "although the results may include synonyms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text1.concordance(\"monstrous\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Displaying 11 of 11 matches:\n",
        "ong the former , one was of a most monstrous size . ... This came towards us , \n",
        "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
        "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
        "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
        "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
        "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
        "th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\n",
        "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
        "ere to enter upon those still more monstrous stories of them which are to be fo\n",
        "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
        "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text1.similar(\"monstrous\")\n",
      "text2.similar(\"monstrous\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "imperial subtly impalpable pitiable curious abundant perilous\n",
        "trustworthy untoward singular lamentable few determined maddens\n",
        "horrible tyrannical lazy mystifying christian exasperate\n",
        "very exceedingly so heartily a great good amazingly as sweet\n",
        "remarkably extremely vast"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text2.common_contexts([\"monstrous\", \"very\"])  # this function takes two arguments"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "a_pretty is_pretty a_lucky am_glad be_glad\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python also lets you create graphs to display data.\n",
      "To represent information about a text graphically, import the Python library *numpy*. We can then generate a dispersion plot that shows where given words occur in a text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "get_ipython().magic(u'matplotlib inline') # allow visuals to show up in this interface---see note below\n",
      "text1.dispersion_plot([\"whale\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEZCAYAAAC5AHPcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1090cf150>"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> **Note**: The second line of the code above is IPython specific, and will not necessarily work in other Python environments!\n",
      "<br>\n",
      "hallenge! Create a dispersion plot for the terms \"citizens\", \"democracy\", \"freedom\", \"duties\" and \"America\" in the innaugural address corpus\n",
      "hat do you think it tells you? \n",
      "\n",
      "codecell>\n",
      "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"]) # plot five words longitudinally"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Quickstart"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've seen a bit now of how NLTK can help you to interrogate a text. Let's back up and talk about Python itself and the environment we're using"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A simple welcome message printer.\n",
      "# Anything after a hash is ignored\n",
      "# Run a cell with shift+enter\n",
      "\n",
      "condition = True\n",
      "if condition is True:\n",
      "    print 'Welcome to Python and the IPython Notebook.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Welcome to Python and the IPython Notebook.\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Success! *And so it begins ... *"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The IPython Notebook"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we start coding, we should familiarise ourselves with the IPython Notebook interface. Click *Help* --\\> *User interface tour* to begin.\n",
      "<br>\n",
      "Keyboard shortcuts come in very handy. Click *Help* --\\> *Keyboard shortcuts* to get an overview. *The more you code, the less you'll want to use your mouse!*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Python: core concepts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you're new to Python, there are a few core concepts that will help you understand how everything works. Here, we'll cover:\n",
      "\n",
      "* Significant whitespace\n",
      "* Input/output types\n",
      "* Commands and arguments\n",
      "* Defining functions\n",
      "* Importing libraries, functions, etc."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Significant Whitespace"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One thing that makes Python unique is that whitespace at the start of the line (use a tab!) is meaningful. In many other language, whitespace at the start of lines is simply a readability convention."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fix this whitespace problem!\n",
      "\n",
      "string = 'user'\n",
      "if string == 'user':\n",
      "print 'Phew, fixed.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, whitespace tells both Python and human readers where things start and stop."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#You should be able to get different kinds of output depending on \n",
      "# how you indent this code.\n",
      "\n",
      "print 'Python\\nis\\n'\n",
      "for i in ['very', 'really', 'truly']:  # repeat three times, quite arbitrarily\n",
      "    print i + '\\n'\n",
      "    if i is 'truly':  # nested conditional\n",
      "        print 'interesting!'\n",
      "    else:\n",
      "        print 'complicated!'\n",
      "#print 'day.'  # at present, this occurs after the three repetitions."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Python\n",
        "is\n",
        "\n",
        "very\n",
        "\n",
        "complicated!\n",
        "really\n",
        "\n",
        "complicated!\n",
        "truly\n",
        "\n",
        "interesting!\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Input/Output Types"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Python understands different *types* of input, including *string*, *integer*, *array*, *item* ... \n",
      "* You need to always make sure your input types are correct, or Python won't know what to do with them.\n",
      "* For example, if you're trying to do maths, everything has to be an integer:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "1 + 2  # integer plus integer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "1 + '2'  # integer plus string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-17-3ce0a44509ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'2'\u001b[0m  \u001b[0;31m# integer plus string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can determine the type of data stored in a variable with type():"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "var = 'A string'\n",
      "print type(var)\n",
      "var = 42\n",
      "print type(var)\n",
      "var = ['item']\n",
      "print type(var)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'str'>\n",
        "<type 'int'>\n",
        "<type 'list'>\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can sometimes easily convert between types."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "secondnumber = '2'\n",
      "1 + int(secondnumber)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Basic syntax"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python has *variables* and *commands*. Commands may have *arguments* and *options*.\n",
      "\n",
      "> IPython highlights your code automatically, which can help you read it faster and spot problems."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import sqrt  # importing math library and square root function\n",
      "avariable = 50  # make a variable that is 50 as integer\n",
      "answer = sqrt(avariable)  # figure out the answer by issuing a command with avariable as an argument\n",
      "print answer  # tell us"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7.07106781187\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This example has two arguments\n",
      "from math import pow  # importing pow function\n",
      "avariable = 50\n",
      "answer = pow(avariable, 3)  # 50 to the power of 3\n",
      "print answer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "125000.0\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Advantages of IPython"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, we've been writing Python code in an IPython notebook. Why?\n",
      "\n",
      "1. The main strength of IPython is that you can run bits of code individually, so you don't have to keep repeating things. In the previous cell, IPython remembered two ingredients in the fruit salad.\n",
      "2. IPython also allows you to display images alongside code, and to save the input and output together.\n",
      "3. IPython also makes learning a bit easier, as mistakes are easier to find and do not break an entire workflow.\n",
      "\n",
      "You can get more information on IPython, including how to install it on your own machine, at the [IPython Homepage](http://ipython.org).\n",
      "\n",
      "> **Note**: not everybody uses *IPython*, so later in the course we'll explain how to convert your work here into 'regular python' scripts."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Defining a function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You may wish to repeat an operation multiple times looking at different texts or different terms within a text. Instead of re-entering the formula every time, you can assign a name and set of actions to a particular task.\n",
      "\n",
      "We've just created a simple function that welcomed you and told you the time.\n",
      "\n",
      "Previously, we calculated the lexical diversity of a text. In NLTK, we can create a function called **lexical diversity** that runs a single line of code. We can then call this function to quickly determine the lexical density of a corpus or subcorpus.\n",
      "Advantages of functions:\n",
      "1. Save you typing\n",
      "2. You can be sure you're doing exactly the same operation every time\n",
      " \n",
      "<markdown cell>\n",
      "*Challenge*\n",
      "Using a function, determine which of the nine texts in the NLTK Book has the highest lexical diversity score."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lexical_diversity(text):\n",
      "    return len(text)/len(set(text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#After the function has been defined, we can run it:\n",
      "lexical_diversity(text2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "20"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The parentheses are important here as they sepatate the the task, that is the work of the function, from the data that the function is to be performed on. The data in parentheses is called the argument of the function. When we use a function, we say that we 'call' it. \n",
      "\n",
      "Other functions that we've used already include len() and sorted() - these were predefined. *lexical_diversity()* is one we set up ourselves; note that it's conventional to put a set of parentheses after a function, to make it clear what we're talking about."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Lists"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python treats a text as a long list of words. First, we'll make some lists of our own, to give you an idea of how a list behaves."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent1 = ['Call', 'me', 'Ishmael', '.']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "['Call', 'me', 'Ishmael', '.']"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(sent1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "4"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The opening sentences of each of our texts have been pre-defined for you. You can inspect them by typing in 'sent2' etc.\n",
      "\n",
      "You can add lists together, creating a new list containing all the items from both lists. You can do this by typing out the two lists or you can add two or more pre-defined lists. This is called concatenation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent4 + sent1\n",
      "\n",
      "# We can also add an item to the end of a list by appending. When we append(), the list itself is updated. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "['Fellow',\n",
        " '-',\n",
        " 'Citizens',\n",
        " 'of',\n",
        " 'the',\n",
        " 'Senate',\n",
        " 'and',\n",
        " 'of',\n",
        " 'the',\n",
        " 'House',\n",
        " 'of',\n",
        " 'Representatives',\n",
        " ':',\n",
        " 'Call',\n",
        " 'me',\n",
        " 'Ishmael',\n",
        " '.']"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent1.append(\"Please\")\n",
      "sent1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "['Call', 'me', 'Ishmael', '.', 'Please']"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are some things we can do to make it easier to read the contents of a string. Note that we get some brackets and so on when we try to print the items in a list as a string."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fruitsalad = []  # declare an empty list\n",
      "fruitsalad.append('watermelon')  # add watermelon\n",
      "fruitsalad.append('orange')  # add orange\n",
      "print 'Our fruit salad contains: ' + str(fruitsalad)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Our fruit salad contains: ['watermelon', 'orange']\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we want to print our ingredients in a nicer looking form, we might use a function like **join**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fruitsalad = []\n",
      "fruitsalad.append('watermelon')\n",
      "fruitsalad.append('orange')\n",
      "listasastring = ''.join(fruitsalad)  # create a string with all the list items joined together\n",
      "print 'Our fruit salad contains: ' + listasastring"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Our fruit salad contains: watermelonorange\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... whoops! Still ugly. We didn't put anything in between the '' to use as a delimiter"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fruitsalad.append('canteloupe')\n",
      "listasastring = ', '.join(fruitsalad)  # note the comma and space in quotation marks\n",
      "print 'Our fruit salad contains: ' + listasastring"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Our fruit salad contains: watermelon, orange, canteloupe\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      " Indexing Lists"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can navigate this list with the help of indexes. Just as we can find out the number of times a word occurs in a text, we can also find where a word first occurs. We can also navigate to different points in a text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4.index('awaken')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "173"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " This works in reverse as well. We can ask Python to locate the 158th item in our list (note that we use square brackets here, not parentheses)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4[158]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "u'the'"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As well as pulling out individual items from a list, indexes can be used to pull out selections of text from a large corpus to inspect. We call this slicing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text5[16715:16735]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "[u'U86',\n",
        " u'thats',\n",
        " u'why',\n",
        " u'something',\n",
        " u'like',\n",
        " u'gamefly',\n",
        " u'is',\n",
        " u'so',\n",
        " u'good',\n",
        " u'because',\n",
        " u'you',\n",
        " u'can',\n",
        " u'actually',\n",
        " u'play',\n",
        " u'a',\n",
        " u'full',\n",
        " u'game',\n",
        " u'without',\n",
        " u'buying',\n",
        " u'it']"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we're asking for the beginning or end of a text, we can leave out the first or second number. For instance, [:5] will give us the first five items in a list while [8:] will give us all the elements from the eighth to the end. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text2[:10]\n",
      "text4[145700:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "[u'upon',\n",
        " u'us',\n",
        " u',',\n",
        " u'we',\n",
        " u'carried',\n",
        " u'forth',\n",
        " u'that',\n",
        " u'great',\n",
        " u'gift',\n",
        " u'of',\n",
        " u'freedom',\n",
        " u'and',\n",
        " u'delivered',\n",
        " u'it',\n",
        " u'safely',\n",
        " u'to',\n",
        " u'future',\n",
        " u'generations',\n",
        " u'.',\n",
        " u'Thank',\n",
        " u'you',\n",
        " u'.',\n",
        " u'God',\n",
        " u'bless',\n",
        " u'you',\n",
        " u'.',\n",
        " u'And',\n",
        " u'God',\n",
        " u'bless',\n",
        " u'the',\n",
        " u'United',\n",
        " u'States',\n",
        " u'of',\n",
        " u'America',\n",
        " u'.']"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To help you understand how indexes work, let's create one.\n",
      "\n",
      "We start by defining the name of our index and then add the items. You probably won't do this in your own work, but you may want to manipulate an index in other ways. Pay attention to the quote marks and commas when you create your test sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "sent[0]\n",
      "sent[8]\n",
      "\n",
      "# Note that the first element in the list is zero. This is because we are telling Python to go zero steps forward in the list. If we use an idnex that is too large (that is, we ask for something that doesn't exist), we'll get an error.\n",
      "\n",
      "# We can modify elements in a list by assigning to one of its index values. We can also replace a slice with new material."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "'dog'"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent[2] = 'furry'\n",
      "sent[7] = 'spotty'\n",
      "sent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "['The', 'quick', 'furry', 'fox', 'jumps', 'over', 'the', 'spotty', 'dog']"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      " Defining variables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In Python, we give the items we're working with names, a process called assignment. For instance, in the NLTK corpus, 'Sense and Sensibility' has been assigned the name 'text2', which is much easier to work with. We also assigend the name 'sent' to the sentence that we created in the previous exercise, so that we could then instruct Python to do various things with it. Assigning a variable in python looks like this:\n",
      "variable = expression\n",
      "You can call your variables (almost) anything you like, but it's a good idea to pick names that will be meaningful and easy to type. You can't use words that already have a meaning in Python, such as import, def, or not. If you try to use a word that is reserved, you'll get a syntax error."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Challenge*\n",
      "1. Create a list called 'Opening' that consists of the phrase \"It was a dark and stormy night; the rain fell in torrents\"\n",
      "2. Create a variable called 'clause' that contains the contents of 'Opening', up to the semi-colon\n",
      "3. Create a variable called 'alphabetised' that sorts 'clause' alphabetically\n",
      "4. Print 'alphabetised' "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "opening = ['It', 'was', 'a', 'dark', 'and', 'stormy', 'night', ';' 'the', 'rain', 'fell', 'in', 'torrents']\n",
      "clause = opening[0:7]\n",
      "alphabetised = sorted(clause)\n",
      "\n",
      "# Note that assigning a variable just causes Python to remember that information without generating any output. If you want Python to show you the result, you have to ask for it (this is a good thing when you assign a variable to a very long list!)."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clause"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "['It', 'was', 'a', 'dark', 'and', 'stormy', 'night']"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alphabetised"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "['It', 'a', 'and', 'dark', 'night', 'stormy', 'was']"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exploring vocabulary 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use Python's ability to perform statistical analysis of data to do further exploration of vocabulary. For instance, we might want to be able to find the most common or least common words in a text. We'll start by looking at frequency distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist1 = FreqDist(text1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist1.most_common(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "[(u',', 18713),\n",
        " (u'the', 13721),\n",
        " (u'.', 6862),\n",
        " (u'of', 6536),\n",
        " (u'and', 6024),\n",
        " (u'a', 4569),\n",
        " (u'to', 4542),\n",
        " (u';', 4072),\n",
        " (u'in', 3916),\n",
        " (u'that', 2982),\n",
        " (u\"'\", 2684),\n",
        " (u'-', 2552),\n",
        " (u'his', 2459),\n",
        " (u'it', 2209),\n",
        " (u'I', 2124),\n",
        " (u's', 1739),\n",
        " (u'is', 1695),\n",
        " (u'he', 1661),\n",
        " (u'with', 1659),\n",
        " (u'was', 1632),\n",
        " (u'as', 1620),\n",
        " (u'\"', 1478),\n",
        " (u'all', 1462),\n",
        " (u'for', 1414),\n",
        " (u'this', 1280),\n",
        " (u'!', 1269),\n",
        " (u'at', 1231),\n",
        " (u'by', 1137),\n",
        " (u'but', 1113),\n",
        " (u'not', 1103),\n",
        " (u'--', 1070),\n",
        " (u'him', 1058),\n",
        " (u'from', 1052),\n",
        " (u'be', 1030),\n",
        " (u'on', 1005),\n",
        " (u'so', 918),\n",
        " (u'whale', 906),\n",
        " (u'one', 889),\n",
        " (u'you', 841),\n",
        " (u'had', 767),\n",
        " (u'have', 760),\n",
        " (u'there', 715),\n",
        " (u'But', 705),\n",
        " (u'or', 697),\n",
        " (u'were', 680),\n",
        " (u'now', 646),\n",
        " (u'which', 640),\n",
        " (u'?', 637),\n",
        " (u'me', 627),\n",
        " (u'like', 624)]"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist1['whale']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "906"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist1.plot(50, cumulative = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEiCAYAAADeViTIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhaAAsBAUABaCAsBCUABYCAoAiwCusvulUyWTagAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x115aa7150>"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Challenge!*\n",
      "\n",
      "Create a function called \"Common_Words\" and use it to compare the 15 most common words of four of the texts in the NLTK book. \n",
      "Discuss what you found with your neighbour\n",
      "\n",
      "codecell>\n",
      "As well as counting individual words, we can count other features of vocabulary, such as how often words of different lengths occur. We do this by putting together a number of the commands we've already learned.\n",
      "We could start like this: [len(w) for w in text1], but this would print the length of every word in the whole book, so let's skip that bit!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist2= FreqDist(len(w) for w in text1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist2.max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist2.freq(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These last two commands tell us that the most common word length is 3, and that these 3 letter words account for about 20% of the book. We can see this just by visually inspecting the list produced by fdist.most_common(), but if this list were too long to inspect readily, or we didn't want to print it, there are others ways to explore it.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is possible to select the longest words in a text, which may tell you something about its vocabulary and style"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "V = set(text4)\n",
      "long_words = [word for word in V if len(word) > 15]\n",
      "sorted(long_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can fine-tune our selection even further by adding further conditions. For instance, we might want to find long words that occur frequently (or rarely)\n",
      "*Challenge!*\n",
      "Can you find all the words in a text that are more than seven letters long and occur more than seven times?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist5 = FreqDist(text5)\n",
      "sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a number of functions defined for NLTK's frequency distributions:\n",
      "\n",
      "* fdist = FreqDist(samples) \tcreate a frequency distribution containing the given samples\n",
      "* fdist[sample] += 1 \tincrement the count for this sample\n",
      "* fdist['monstrous'] \tcount of the number of times a given sample occurred\n",
      "* fdist.freq('monstrous') \tfrequency of a given sample\n",
      "* fdist.N() \ttotal number of samples\n",
      "* fdist.most_common(n) \tthe n most common samples and their frequencies\n",
      "* for sample in fdist: \titerate over the samples\n",
      "* fdist.max() \tsample with the greatest count\n",
      "* fdist.tabulate() \ttabulate the frequency distribution\n",
      "* fdist.plot() \tgraphical plot of the frequency distribution\n",
      "* fdist.plot(cumulative=True) \tcumulative plot of the frequency distribution\n",
      "* fdist1 |= fdist2 \tupdate fdist1 with counts from fdist2\n",
      "* fdist1 < fdist2 \ttest if samples in fdist1 occur less frequently than in fdist2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also find words that typically occur together, which tend to be very specific to a text or genre of texts. We'll talk more about these features and how to use them later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4.collocations()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "United States; fellow citizens; four years; years ago; Federal\n",
        "Government; General Government; American people; Vice President; Old\n",
        "World; Almighty God; Fellow citizens; Chief Magistrate; Chief Justice;\n",
        "God bless; every citizen; Indian tribes; public debt; one another;\n",
        "foreign nations; political parties\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also use numerical operators to refine the types of searches we ask Python to run. We can use the following relational operators:\n",
      "\n",
      "* <       less than\n",
      "* <=      less than or equal to\n",
      "* ==      equal to (note this is two \"=\" signs, not one)\n",
      "* !=      not equal to\n",
      "* \\>      greater than\n",
      "* \\>=     greater than or equal to"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Challenge!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using one of the pre-defined sentences in the NLTK corpus, use the relational operators above to find:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Words longer than four characters\n",
      "2. Words of four or more characters\n",
      "3. Words of exactly four characters\n",
      "\n",
      "We can also look for features such as letter combinations, upper and lowercase letters, and digits. some operators you might like to use are:\n",
      "\n",
      "* s.startswith(t) test if s starts with t\n",
      "* s.endswith(t) \ttest if s ends with t\n",
      "* t in s \t        test if t is a substring of s\n",
      "* s.islower() \ttest if s contains cased characters and all are lowercase\n",
      "* s.isupper() \ttest if s contains cased characters and all are uppercase\n",
      "* s.isalpha() \ttest if s is non-empty and all characters in s are alphabetic\n",
      "* s.isalnum() \ttest if s is non-empty and all characters in s are alphanumeric\n",
      "* s.isdigit() \ttest if s is non-empty and all characters in s are digits\n",
      "* s.istitle() \ttest if s contains cased characters and is titlecased (i.e. all words in s have initial capitals)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(w for w in set(text1) if w.endswith('ableness'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "[u'comfortableness',\n",
        " u'honourableness',\n",
        " u'immutableness',\n",
        " u'indispensableness',\n",
        " u'indomitableness',\n",
        " u'intolerableness',\n",
        " u'palpableness',\n",
        " u'reasonableness',\n",
        " u'uncomfortableness']"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(n for n in sent7 if n.isdigit())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "['29', '61']"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Bonus!**\n",
      "\n",
      "You'll remember right at the beginning we started looking at the size of the vocabulary of a text, but there were two problems with the results we got from using **len(set(text1)**. \n",
      "\n",
      "his count includes items of punctuation and treats capitalised and non-capitalised words as different things (*This* vs *this*). We can now fix this. We can start by getting rid of capitalised words, then we can get rid of the punctuation and numbers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(word.lower() for word in text1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "17231"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(word.lower() for word in text1 if word.isalpha()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "16948"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img style=\"float:left\" src=\"http://images.catsolonline.com/cache/custom/CEN/CE651-250x250.jpg\" />\n",
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You've completed the first session. To summarise, here's what you've learned so far:\n",
      "\n",
      "* How to navigate the iPython notebook\n",
      "* How Python uses whitespace \n",
      "* Why functions are useful and how to define one\n",
      "* How to use basic Pyton commands to start exploring features of a text\n",
      "\n",
      "We'll practice all these commands in the following lessons ... so don't panic!\n",
      "\n",
      "It's a lot to take in and it will probably take a while before you feel really comfortable.\n",
      "\n",
      "*See you after the break!*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Session 2: Common NLTK tasks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "In this session we provide an quick introduction to the field of *corpus linguistics*. We then engage with common uses of NLTK within these areas, such as sentence segmentation, tokenisation and stemming. Often, NLTK has inbuilt methods for performing these tasks. As a learning exercise, however, we will sometimes build basic tools from scratch."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Corpus linguistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Though corpus linguistics has been around since the 1950s, it is only in the last 20 years that its methods have been made available to individual researchers. GUIs including [Wordsmith Tools](http://www.lexically.net/wordsmith/) and [AntConc](http://www.laurenceanthony.net/software.html). \n",
      "\n",
      "Alongside the development of GUIs, there has also been a shift from *general, balanced corpora* (corpora seeking to represent a language generally) toward *specialised corpora* (corpora containing texts of one specific type, from one speaker, etc.). More and more commonly, texts are taken from the Web.\n",
      "\n",
      "> **Note:** We'll discuss building corpora from online texts in Session 6.\n",
      "\n",
      "After a long period of resistance, corpus linguistics has gained acceptence within a number of research areas. A few popular applications are within:\n",
      "\n",
      "* **Lexicography** (creating usage-based definitions of words and locating real examples)\n",
      "* **Language pedagogy** (advanced language learners can use a concordancing GUI or collocation tests to understand how certain words are used in the target language)\n",
      "* **Discourse analysis** (researching how meaning is made beyond the level of the clause/sentence)\n",
      "\n",
      "Notably, corpus linguistic methods have been embraced within the emerging paradigm of Digital Humanities, often under the banner of *data carpentry* or *distance reading*."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Corpora and discourse"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As hardware, software and data become more and more available, people have started using corpus linguistic methods for discourse-analytic work. Paul Baker refers the combination of corpus linguistics and (critical) discourse analysis as a *useful methodological synergy*. Corpora bring objectivity and empiricism to a qualitative, interpretative tradition, while discourse-analytic methods provide corpus linguistics with a means of contextualising abstracted results.\n",
      "\n",
      "Within this area, researchers rely on corpora to varying extents. In *corpus-driven* discourse analysis, researchers interpret the corpus based on the findings of the corpus interrogation. In *corpus-assisted* discourse analysis, researchers may use corpora to provide evidence about the way a given person/idea/discourse is commonly represented by certain people/in certain publications etc.\n",
      "\n",
      "Our work here falls under the *corpus-driven* heading, as we are exploring the dataset without any major hypotheses in mind.\n",
      "\n",
      "> **Note:** Some linguists remain skeptical of corpus linguistics generally. In a well-known critique, Henry Widdowson ([2000, p. 6-7](#ref:widdowson)) said:\n",
      ">\n",
      "> Corpus linguistics \\[...\\] is no doubt that this is an immensely important development in descriptive linguistics. That is not the issue here. The quantitative analysis of text by computer reveals facts about actual language behaviour which are not, or at least not immediately, accessible to intuition. There are frequencies of occurrence of words, and regular patterns of collocational co-occurrence, which users are unaware of, though they must be part of their competence in a procedural sense since they would not otherwise be attested. They are third person observed data ('When do they use the word X?') which are different from the first person data of introspection ('When do I use the word X?'), and the second person data of elicitation ('When do you use the word X?'). Corpus analysis reveals textual facts, fascinating profiles of produced language, and its concordances are always springing surprises. They do indeed reveal a reality about language usage which was hitherto not evident to its users.\n",
      ">\n",
      "> But this achievement of corpus analysis at the same time necessarily defines its limitations. For one thing, since what is revealed is contrary to intuition, then it cannot represent the reality of first person awareness. We get third person facts of what people do, but not the facts of what people know, nor what they think they do: they come from the perspective of the observer looking on, not the introspective of the insider. In ethnomethodogical terms, we do not get member categories of description. Furthermore, it can only be one aspect of what they do that is captured by such quantitative analysis. For, obviously enough, the computer can only cope with the material products ofwhat people do when they use language. It can only analyse the textual traces of the processes whereby meaning is achieved: it cannot account for the complex interplay of linguistic and contextual factors whereby discourse is enacted. It cannot produce ethnographic descriptions of language use. In reference to Hymes's components of communicative competence (Hymes 1972), we can say that corpus analysis deals with the textually attested, but not with the encoded possible, nor the contextually appropriate.\n",
      "> \n",
      "> To point out these rather obvious limitations is not to undervalue corpus analysis but to define more clearly where its value lies. What it can do is reveal the properties of text, and that is impressive enough. But it is necessarily only a partial account of real language. For there are certain aspects of linguistic reality that it cannot reveal at all. In this respect, the linguistics of the attested is just as partial as the linguistics of the possible."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Loading a corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we have to load a corpus. We'll use a text file containing posts to an Australian forum. This file is available online, at the [ResBaz Github](https://github.com/resbaz). We can ask Python to get it for us. \n",
      "\n",
      "> Later in the course, we'll discuss how to extract data from the Web and turn this data into a corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib import urlopen # a library for working with urls\n",
      "url = \"https://raw.githubusercontent.com/resbaz/lessons/master/nltk/corpora/oz_politics/ozpol.txt\" # provide a url\n",
      "raw = urlopen(url).read() # download and read the corpus into raw variable\n",
      "raw = unicode(raw, 'utf-8')\n",
      "print len(raw) # how many characters does it contain?\n",
      "print raw[:2000] # first 2000 characters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "142575\n",
        "No Greens-win, many of us right wingers want to stay the hell out of the Middle East. Nothing is going to stop that sh.thole of the world from tearing each others' throats out.\n",
        "After all, they have been doing it successfully for centuries.\n",
        "Then you better start educating your hard right mates about Greens' renewable energy. Sooner we end our addiction to arab oil and middle eastern exports, like live animals, the sooner we can distance ourselves.\n",
        "I am not saying that all Muslims are like this. However, I am saying that many, many of them are. They are flying under the radar and getting strong, just like Hitler's Storm Troopers before little Adolph rose to power.\n",
        "Sensationalist right? No one thought much about little Adolph and his brown shirted followers, until they stormed to power, whilst the do gooders stood by and tut tutted.\n",
        "If you are a student of history you can see exactly what is happening here. Soon it will be too late, it most probably already is.\n",
        "Just when Sydney-siders thought it was safe to close their eyes and go to sleep for a night, pondering the lack of evening gun battles lately, they learn of something worse than drive by shootings.\n",
        "As they woke to another day\u0092s work the people of Sydney [and Melbourne] learned of a joint operation between their States\u0092 Police forces and the Australian Federal Police, against an imminent terrorist attack in their cities.\n",
        "Four men - all Australian citizens - were arrested this morning as federal and state police, armed with search warrants, swooped on members of the suspected terror cell this morning in the second-largest counter-terrorism operation in the nation's history\u0085",
        ". About 400 police raided homes in the northern Melbourne suburbs of Glenroy, Meadow Heights, Roxburgh Park, Broadmeadows, Westmeadows, Preston and Epping. They also raided homes at Carlton in inner Melbourne and Colac in southwestern Victoria. (source)\n",
        "The intention, apparently, was to attack a Sydney army base:\n",
        "Authorities believe the group is\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We actually already downloaded this file when we first cloned the ResBaz GitHub repository. It's in our *corpora* folder. We can access it like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('corpora/oz_politics/ozpol.txt')\n",
      "raw = f.read()\n",
      "raw = unicode(raw, 'utf-8') # make it into unicode!\n",
      "len(raw)\n",
      "print len(raw)\n",
      "print raw[:2000] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "142575\n",
        "No Greens-win, many of us right wingers want to stay the hell out of the Middle East. Nothing is going to stop that sh.thole of the world from tearing each others' throats out.\n",
        "After all, they have been doing it successfully for centuries.\n",
        "Then you better start educating your hard right mates about Greens' renewable energy. Sooner we end our addiction to arab oil and middle eastern exports, like live animals, the sooner we can distance ourselves.\n",
        "I am not saying that all Muslims are like this. However, I am saying that many, many of them are. They are flying under the radar and getting strong, just like Hitler's Storm Troopers before little Adolph rose to power.\n",
        "Sensationalist right? No one thought much about little Adolph and his brown shirted followers, until they stormed to power, whilst the do gooders stood by and tut tutted.\n",
        "If you are a student of history you can see exactly what is happening here. Soon it will be too late, it most probably already is.\n",
        "Just when Sydney-siders thought it was safe to close their eyes and go to sleep for a night, pondering the lack of evening gun battles lately, they learn of something worse than drive by shootings.\n",
        "As they woke to another day\u0092s work the people of Sydney [and Melbourne] learned of a joint operation between their States\u0092 Police forces and the Australian Federal Police, against an imminent terrorist attack in their cities.\n",
        "Four men - all Australian citizens - were arrested this morning as federal and state police, armed with search warrants, swooped on members of the suspected terror cell this morning in the second-largest counter-terrorism operation in the nation's history\u0085",
        ". About 400 police raided homes in the northern Melbourne suburbs of Glenroy, Meadow Heights, Roxburgh Park, Broadmeadows, Westmeadows, Preston and Epping. They also raided homes at Carlton in inner Melbourne and Colac in southwestern Victoria. (source)\n",
        "The intention, apparently, was to attack a Sydney army base:\n",
        "Authorities believe the group is\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Regular Expressions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we go any further, we need to talk about Regular Expressions. Regular Expressions (regexes) are ways of searching for complex patterns in strings. Regexes are standardised across many programming platforms, and can also be used in GUI text editors and word processers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk # just in case\n",
      "import re # import this before using regexes!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If only using alphanumeric characters and spaces, regexes work like any normal search."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print only match\n",
      "regex = re.compile(r\"government\")\n",
      "re.findall(regex, raw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "[u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government',\n",
        " u'government']"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hmm ... not very useful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print whole line\n",
      "regex = re.compile(r'government')\n",
      "for line in raw.splitlines():\n",
      "    if regex.search(line) is not None:\n",
      "        print line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "It did not include casualties from government-backed action such as aerial bombing or other killings.\"\n",
        "It was only 20 years ago that Muslims of a certain region were asking Australian government to send troops to their dung heap of a country. Wish I can remember which one it was. General consensus was that no one was going to take those requests seriously -- unless requested by the British or American governments. Even then, I bet the public would have laughed it off.\n",
        "As for Big To - of course the only thing that might save him and his government would be a war of some kind... worked for his heroine Maggie thatcher, whose government promptly hid away the wounded veterans of that conflict so as not to upset the public.\n",
        "Terrorist attack = the unlawful use of force or violence against persons or property to intimidate or coerce a government, the civilian population, or any segment thereof in furtherance of political or social objectives.\n",
        "An ADF spokesman was on 7.30 the other night trying to pitch the government's message. The message he was selling was that we need to defend \"Western values\" and our alliance with the US. He repeated these things a number of times in the interview, which told me he was following the government's script.\n",
        "The Middle East is in the situation it is today thanks to the Western support for dictatorships and direct military involvement. The Middle East is in this situation because very few Arab states have made the transition to popularly elected governments. The Middle East is still a collection of tribes and tribal allegiances, with very few stable republics. Rather than fostering and strengthening local systems of governance and popular representation, Western intervention has merely backed tribes, trained their armies and security forces, and let them do their job of supressing their populations rather than including them. The most recent example of this is the al Malaki government in Iraq, backed all the way by Uncle.\n",
        "Mind you, the bombing of Serbia did lead to the fall of Milosevic, but his government was not an insurgency. What's happening in Syria and Iraq right now is not conventional war. It's guerrilla war, and for this you need an entirely different approach to knee-jerk displays of force.\n",
        "It's very convenient for the government to publicise people being charged with \"buying bombs\".\u00a0 \u00a0Don't you think?\n",
        "Political policies ... like the policy of the howard government to invade Iraqi illegally.\n",
        "The federal government will have enhanced powers to suspend passports at short notice to stem the flow of young Australians joining the Islamist rebellions in Syria and Iraq, under legislation expected to be put to Parliament shortly.\n",
        "The reforms, which have strong support in the counter-terrorism community and from government, will plug a security gap at Australia\u0092s borders that is being exploited by scores of would-be jihadists.\n",
        "The government is also weighing whether it can revoke the Australian passports of dual nationals fighting abroad, although this is yet to be finalised and is proving problematic to implement given Australia\u0092s obligations under UN treaties.\n",
        "The Arab Spring was the natural consequence of the end of the Cold War. Without well-organised puppet governments in countries like Egypt, Lybia and Tunisia, their populations voted with their own feet (or weapons). Either way, they got to vote for the first time.\n",
        "The issue of genocide is an entirely different issue. It's possible for us to protect local civilians or arm them to protect themselves, but this is not sustainable long-term. Ultimately, this is a job best done by local forces who are able to get in quick. The Kurds and Turks have already acted to do this. The Iraqis - a government and army we suported, trained and set up - have proven to be incapable of doing this.\n",
        "Which is exactly what the government was saying last week.\n",
        "The government's sole rationale to ISIS being an Australian regional issue is that returned fighters trained in explosives and firearms use would be an internal risk.\n",
        "David Johnston had no answer to this. Any returned fighter is, as the government keeps arguing, subject to Australian laws for supporting terrorism and fighting in foreign wars.\n",
        "The government are currently mouthing motherhood statements and platitudes - \"humanitarian\", \"global partnerships\", \"restoring international security\".\n",
        "It's a pretty simple question all citizens should want to know if their government is sending their country to war: why?\n",
        "And i demand that the Australian government provide me withasylum [protection from fear] from the threats coming from moslems in our Australian community......because i know that all moslems, are moslems.\n",
        "And i demand that the Australian government provide me with asylum [protection from fear] from moslems who are walking on the streets of our Australian community.\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... but regex can be much more powerful than that:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regex = re.compile(r\"[A-Za-z]+ment\")\n",
      "results = re.findall(regex, raw)\n",
      "sorted(set(results)) # sort and print only unique results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "[u'Advertisement',\n",
        " u'Government',\n",
        " u'Internment',\n",
        " u'Parliament',\n",
        " u'agreement',\n",
        " u'argument',\n",
        " u'armament',\n",
        " u'assessment',\n",
        " u'comment',\n",
        " u'commitment',\n",
        " u'document',\n",
        " u'element',\n",
        " u'encampment',\n",
        " u'environment',\n",
        " u'ferment',\n",
        " u'fundament',\n",
        " u'government',\n",
        " u'implement',\n",
        " u'impliment',\n",
        " u'internment',\n",
        " u'investment',\n",
        " u'involvement',\n",
        " u'moment',\n",
        " u'movement',\n",
        " u'pronouncement',\n",
        " u'punishment',\n",
        " u'resentment',\n",
        " u'segment',\n",
        " u'sentiment',\n",
        " u'statement']"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you want to search for any special character, it must be 'escaped' by a backslash:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string = 'What!? :) U fink im flirtin wit *u*!? LOL'\n",
      "regex = re.compile(r'\\*u\\*!\\?') # asterisks and question marks need to be escaped, but not exclamation marks.\n",
      "re.findall(regex, string)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "['*u*!?']"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are some additional resources:\n",
      "\n",
      "* [Regex Info](http://www.regular-expressions.info/): tutorials etc.\n",
      "* [Regexr](http://www.regexr.com/): a place to build and test out your regex\n",
      "* [Regex crosswords](http://regexcrossword.com/): exactly what you think it is!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below will get any word over *minlength* alphabetic characters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "minlength = 5  # minimum number of letters as integer gets passed into the building of the Regex\n",
      "sentence = 'We, the democratically-elected leaders of our people, hereby declare Kosovo to be an independent and sovereign state'\n",
      "pattern = re.compile(r'[A-Za-z]{' + str(minlength) + ',}') \n",
      "# What happens if you add a hyphen after 'a-z' inside the square brackets above?\n",
      "re.findall(pattern, sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "['democratically',\n",
        " 'elected',\n",
        " 'leaders',\n",
        " 'people',\n",
        " 'hereby',\n",
        " 'declare',\n",
        " 'Kosovo',\n",
        " 'independent',\n",
        " 'sovereign',\n",
        " 'state']"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sentence segmentation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, with a basic understanding of regex, we can now start to turn our corpus into a structured resource. At present, we have 'raw', a very, very long string of text.\n",
      "\n",
      " We should break the string into segments. First, we'll split the corpus into sentences. This task is a pretty boring one, and it's tough for us to improve on existing resources. We'll try, though.\n",
      "\n",
      "Let's define a sentence as any string ending with (one or more) newline, full stop, question mark, or exclamation mark. A regex can be written to find these, and the split() function can  be used to break the string into a list of strings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "sentences = re.split(\"(\\n|\\.|\\?|!)+\", raw)\n",
      "print 'Sentence: ' + '\\nSentence: '.join(sentences[:10]) # print the first ten sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Sentence: No Greens-win, many of us right wingers want to stay the hell out of the Middle East\n",
        "Sentence: .\n",
        "Sentence:  Nothing is going to stop that sh\n",
        "Sentence: .\n",
        "Sentence: thole of the world from tearing each others' throats out\n",
        "Sentence: \n",
        "\n",
        "Sentence: After all, they have been doing it successfully for centuries\n",
        "Sentence: \n",
        "\n",
        "Sentence: Then you better start educating your hard right mates about Greens' renewable energy\n",
        "Sentence: .\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well, it worked, sort of. What problems do we have?\n",
      "\n",
      "1. sh.thole was split\n",
      "2. Empty matches/punctuation only matches\n",
      "\n",
      "So, to fix our first problem, we have to make an intpretative decision. Is *sh.thole* a word? We could:\n",
      "\n",
      "1. Clean the corpus and remove this false positive\n",
      "2. Define sentence differently\n",
      "\n",
      "We'll go with the second option, for better or worse. The regex in the code below makes sure that any sentence final character must not be followed by a letter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences = re.split(\"(?:\\n|\\.|\\?|!)+([^a-zA-Z]|$)\", raw)\n",
      "print 'Sentence: ' + '\\nSentence: '.join(sentences[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Sentence: No Greens-win, many of us right wingers want to stay the hell out of the Middle East\n",
        "Sentence:  \n",
        "Sentence: Nothing is going to stop that sh.thole of the world from tearing each others' throats out\n",
        "Sentence: \n",
        "\n",
        "Sentence: After all, they have been doing it successfully for centuries\n",
        "Sentence: \n",
        "\n",
        "Sentence: Then you better start educating your hard right mates about Greens' renewable energy\n",
        "Sentence:  \n",
        "Sentence: Sooner we end our addiction to arab oil and middle eastern exports, like live animals, the sooner we can distance ourselves\n",
        "Sentence: \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Problem 1 is solved: *sh.thole* is now a single token. To fix Problem 2, we will remove list items not containing a letter, as any sentence must contain at least one by definition."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regex = re.compile('[a-zA-Z]')\n",
      "no_blanklines = [sentence for sentence in sentences if regex.match(sentence)]\n",
      "# or, an alternative approach using filter function:\n",
      "# no_blanklines = filter(regex.match, sentences)\n",
      "print 'Sentence: ' + '\\nSentence: '.join(no_blanklines[:20]) # this should be better!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Sentence: No Greens-win, many of us right wingers want to stay the hell out of the Middle East\n",
        "Sentence: Nothing is going to stop that sh.thole of the world from tearing each others' throats out\n",
        "Sentence: After all, they have been doing it successfully for centuries\n",
        "Sentence: Then you better start educating your hard right mates about Greens' renewable energy\n",
        "Sentence: Sooner we end our addiction to arab oil and middle eastern exports, like live animals, the sooner we can distance ourselves\n",
        "Sentence: I am not saying that all Muslims are like this\n",
        "Sentence: However, I am saying that many, many of them are\n",
        "Sentence: They are flying under the radar and getting strong, just like Hitler's Storm Troopers before little Adolph rose to power\n",
        "Sentence: Sensationalist right\n",
        "Sentence: No one thought much about little Adolph and his brown shirted followers, until they stormed to power, whilst the do gooders stood by and tut tutted\n",
        "Sentence: If you are a student of history you can see exactly what is happening here\n",
        "Sentence: Soon it will be too late, it most probably already is\n",
        "Sentence: Just when Sydney-siders thought it was safe to close their eyes and go to sleep for a night, pondering the lack of evening gun battles lately, they learn of something worse than drive by shootings\n",
        "Sentence: As they woke to another day\u0092s work the people of Sydney [and Melbourne] learned of a joint operation between their States\u0092 Police forces and the Australian Federal Police, against an imminent terrorist attack in their cities\n",
        "Sentence: Four men - all Australian citizens - were arrested this morning as federal and state police, armed with search warrants, swooped on members of the suspected terror cell this morning in the second-largest counter-terrorism operation in the nation's history\u0085",
        "\n",
        "Sentence: About 400 police raided homes in the northern Melbourne suburbs of Glenroy, Meadow Heights, Roxburgh Park, Broadmeadows, Westmeadows, Preston and Epping\n",
        "Sentence: They also raided homes at Carlton in inner Melbourne and Colac in southwestern Victoria\n",
        "Sentence: It is understood the men plan to kill as many soldiers as possible before they are themselves killed\n",
        "Sentence: Members of the group have been observed carrying out surveillance of Holsworthy Barracks in western Sydney and other suspicious activity around defence bases in Victoria\n",
        "Sentence: Electronic surveillance on the suspects is believed to have picked up discussions about ways to obtain weapons to carry out what would be the worst terror attack on Australian soil\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below turns out sentence segmenter into a function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sent_seg(string):\n",
      "    allmatches = re.split(\"(?:\\n|\\.|\\?|!)+([^a-zA-Z]|$)\", string)\n",
      "    regex = re.compile('[a-zA-Z]')\n",
      "    sentences = [sentence for sentence in allmatches if regex.match(sentence)]\n",
      "    length = len(sentences)\n",
      "    return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Call the segmenter on our raw text here. Store it in a variable called 'sents'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's all academic, anyway: NLTK actually has a sentence segmenter built in that works better than ours (we didn't deal with quotation marks, or brackets, for example)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pprint # pretty printer\n",
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "sents = sent_tokenizer.tokenize(raw)\n",
      "pprint.pprint(sents[101:111]) # another way to print list items nicely"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Post some better information.',\n",
        " u\"There's been hundreds in Iraq alone.\",\n",
        " u'The Iraq terrorist attacks are reported.',\n",
        " u\"You need to look harder:\\nJihadists issuing their warning that the beheadings will continue if the U.S. continues to support the KurdsHours after shocking world with desert execution of 300 Syrian soldiers, ISIS parade Kurds dressed in Guantanamo-style suits, behead one and promise to kill them all unless USA pull out of Iraq\\nSurvivor: Mohammed, whose name has been changed out of fears for his safety, was forced to attend an Islamic State children's campSurvivor of ISIS children's camp reveals how young boys are whipped and made to watch men being crucified and women stoned to death\\nIn the last 10 or 15 years, most of (more than half) the terror attacks/deaths have been the result of Islamic Extremists.\",\n",
        " u'In the last 10 to 15 years, most (more than half) terrorist attacks have been carried out by non-Muslims.',\n",
        " u\"I've already provided the proof of this.\",\n",
        " u\"If you refuse to believe, that's your problem.\",\n",
        " u'Go on crossing the road every time you see a Muslim on your side of the street, if that makes you feel safer.',\n",
        " u\"No, actually you have NOT..your 'proof' ends in 2005, which is 9 years ago..how many non-mulim terror attacks happened between 2005 and 2014, and how many muslim terror attacks happened in the same period??\",\n",
        " u'Well just for the July section of that Wiki link, about 26 out of 31 attacks were Muslim inspired attacks.']\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alright, we have sentences. Now what?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Tokenisation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tokenisation is simply the process of breaking texts down into words. We already did a little bit of this in Session 1. We won't build our own tokenizer, because it's not much fun. NLTK has one we can rely on.\n",
      "\n",
      "Keep in mind that definitions of tokens are not standardised, especially for languages other than English. Serious problems arise when comparing two corpora that have been tokenised differently.\n",
      "\n",
      "> **Note:** It is also possible to use NLTK to break tokens into morphemes, syllables, or phonemes. We're not going to go down those roads, though."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenized_sents = [nltk.word_tokenize(i) for i in sents]\n",
      "print tokenized_sents[:10]\n",
      "# another view:\n",
      "# pprint.pprint(tokenized_sents[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[u'No', u'Greens-win', u',', u'many', u'of', u'us', u'right', u'wingers', u'want', u'to', u'stay', u'the', u'hell', u'out', u'of', u'the', u'Middle', u'East', u'.'], [u'Nothing', u'is', u'going', u'to', u'stop', u'that', u'sh.thole', u'of', u'the', u'world', u'from', u'tearing', u'each', u'others', u\"'\", u'throats', u'out', u'.'], [u'After', u'all', u',', u'they', u'have', u'been', u'doing', u'it', u'successfully', u'for', u'centuries', u'.'], [u'Then', u'you', u'better', u'start', u'educating', u'your', u'hard', u'right', u'mates', u'about', u'Greens', u\"'\", u'renewable', u'energy', u'.'], [u'Sooner', u'we', u'end', u'our', u'addiction', u'to', u'arab', u'oil', u'and', u'middle', u'eastern', u'exports', u',', u'like', u'live', u'animals', u',', u'the', u'sooner', u'we', u'can', u'distance', u'ourselves', u'.'], [u'I', u'am', u'not', u'saying', u'that', u'all', u'Muslims', u'are', u'like', u'this', u'.'], [u'However', u',', u'I', u'am', u'saying', u'that', u'many', u',', u'many', u'of', u'them', u'are', u'.'], [u'They', u'are', u'flying', u'under', u'the', u'radar', u'and', u'getting', u'strong', u',', u'just', u'like', u'Hitler', u\"'s\", u'Storm', u'Troopers', u'before', u'little', u'Adolph', u'rose', u'to', u'power', u'.'], [u'Sensationalist', u'right', u'?'], [u'No', u'one', u'thought', u'much', u'about', u'little', u'Adolph', u'and', u'his', u'brown', u'shirted', u'followers', u',', u'until', u'they', u'stormed', u'to', u'power', u',', u'whilst', u'the', u'do', u'gooders', u'stood', u'by', u'and', u'tut', u'tutted', u'.']]\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stemming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stemming is the task of finding the stem of a word. So, *cats --> cat*, or *taking --> take*. It is an important task when counting words, as often the counting each inflection seperately is not particuarly helpful: forms of the verb 'to be' might seem under-represented if we could *is, are, were, was, am, be, being, been* separately. \n",
      "\n",
      "NLTK has pre-programmed stemmers, but we can build our own using some of the skills we've already learned.\n",
      "\n",
      "A stemmer is the kind of thing that would make a good function, so let's do that."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stem(word):\n",
      "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']: # list of suffixes\n",
      "        if word.endswith(suffix):\n",
      "            return word[:-len(suffix)] # delete the suffix\n",
      "    return word"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's run it over some text and see how it performs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# empty list for our output\n",
      "stemmed_sents = []\n",
      "for sent in tokenized_sents:\n",
      "    # empty list for stemmed sentence:\n",
      "    stemmed = []\n",
      "    for word in sent:\n",
      "        # append the stem of every word\n",
      "        stemmed.append(stem(word))\n",
      "    # append the stemmed sentence to the list of sentences\n",
      "    stemmed_sents.append(stemmed)\n",
      "# pretty print the output\n",
      "pprint.pprint(stemmed_sents[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[u'No',\n",
        "  u'Greens-win',\n",
        "  u',',\n",
        "  u'many',\n",
        "  u'of',\n",
        "  u'u',\n",
        "  u'right',\n",
        "  u'winger',\n",
        "  u'want',\n",
        "  u'to',\n",
        "  u'stay',\n",
        "  u'the',\n",
        "  u'hell',\n",
        "  u'out',\n",
        "  u'of',\n",
        "  u'the',\n",
        "  u'Middle',\n",
        "  u'East',\n",
        "  u'.'],\n",
        " [u'Noth',\n",
        "  u'i',\n",
        "  u'go',\n",
        "  u'to',\n",
        "  u'stop',\n",
        "  u'that',\n",
        "  u'sh.thole',\n",
        "  u'of',\n",
        "  u'the',\n",
        "  u'world',\n",
        "  u'from',\n",
        "  u'tear',\n",
        "  u'each',\n",
        "  u'other',\n",
        "  u\"'\",\n",
        "  u'throat',\n",
        "  u'out',\n",
        "  u'.'],\n",
        " [u'After',\n",
        "  u'all',\n",
        "  u',',\n",
        "  u'they',\n",
        "  u'have',\n",
        "  u'been',\n",
        "  u'do',\n",
        "  u'it',\n",
        "  u'successful',\n",
        "  u'for',\n",
        "  u'centur',\n",
        "  u'.'],\n",
        " [u'Then',\n",
        "  u'you',\n",
        "  u'better',\n",
        "  u'start',\n",
        "  u'educat',\n",
        "  u'your',\n",
        "  u'hard',\n",
        "  u'right',\n",
        "  u'mat',\n",
        "  u'about',\n",
        "  u'Green',\n",
        "  u\"'\",\n",
        "  u'renewable',\n",
        "  u'energy',\n",
        "  u'.'],\n",
        " [u'Sooner',\n",
        "  u'we',\n",
        "  u'end',\n",
        "  u'our',\n",
        "  u'addiction',\n",
        "  u'to',\n",
        "  u'arab',\n",
        "  u'oil',\n",
        "  u'and',\n",
        "  u'middle',\n",
        "  u'eastern',\n",
        "  u'export',\n",
        "  u',',\n",
        "  u'like',\n",
        "  u'l',\n",
        "  u'animal',\n",
        "  u',',\n",
        "  u'the',\n",
        "  u'sooner',\n",
        "  u'we',\n",
        "  u'can',\n",
        "  u'distance',\n",
        "  u'ourselv',\n",
        "  u'.'],\n",
        " [u'I',\n",
        "  u'am',\n",
        "  u'not',\n",
        "  u'say',\n",
        "  u'that',\n",
        "  u'all',\n",
        "  u'Muslim',\n",
        "  u'are',\n",
        "  u'like',\n",
        "  u'thi',\n",
        "  u'.'],\n",
        " [u'However',\n",
        "  u',',\n",
        "  u'I',\n",
        "  u'am',\n",
        "  u'say',\n",
        "  u'that',\n",
        "  u'many',\n",
        "  u',',\n",
        "  u'many',\n",
        "  u'of',\n",
        "  u'them',\n",
        "  u'are',\n",
        "  u'.'],\n",
        " [u'They',\n",
        "  u'are',\n",
        "  u'fly',\n",
        "  u'under',\n",
        "  u'the',\n",
        "  u'radar',\n",
        "  u'and',\n",
        "  u'gett',\n",
        "  u'strong',\n",
        "  u',',\n",
        "  u'just',\n",
        "  u'like',\n",
        "  u'Hitler',\n",
        "  u\"'\",\n",
        "  u'Storm',\n",
        "  u'Trooper',\n",
        "  u'before',\n",
        "  u'little',\n",
        "  u'Adolph',\n",
        "  u'rose',\n",
        "  u'to',\n",
        "  u'power',\n",
        "  u'.'],\n",
        " [u'Sensationalist', u'right', u'?'],\n",
        " [u'No',\n",
        "  u'one',\n",
        "  u'thought',\n",
        "  u'much',\n",
        "  u'about',\n",
        "  u'little',\n",
        "  u'Adolph',\n",
        "  u'and',\n",
        "  u'hi',\n",
        "  u'brown',\n",
        "  u'shirt',\n",
        "  u'follower',\n",
        "  u',',\n",
        "  u'until',\n",
        "  u'they',\n",
        "  u'storm',\n",
        "  u'to',\n",
        "  u'power',\n",
        "  u',',\n",
        "  u'whilst',\n",
        "  u'the',\n",
        "  u'do',\n",
        "  u'gooder',\n",
        "  u'stood',\n",
        "  u'by',\n",
        "  u'and',\n",
        "  u'tut',\n",
        "  u'tutt',\n",
        "  u'.']]\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking at the output, we can see that the stemmer works: *wingers* becomes *winger*, and *tearing* becomes *tear*. But, sometimes it does things we don't want: *Nothing* becomes *noth*, and *mate* becomes *mat*. Even so, for the learns, let's rewrite our function with a regex:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stem(word):\n",
      "    import re\n",
      "    regex = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
      "    stem, suffix = re.findall(regex, word)[0]\n",
      "    return stem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because we just redefined the *stem()* function, we can run the previous code and get different results."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's a very quick implemenation of our stemmer on our raw tokens:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = nltk.word_tokenize(raw)\n",
      "stemmed = [stem(t) for t in tokens]\n",
      "print stemmed[:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'No', u'Greens-win', u',', u'many', u'of', u'u', u'right', u'winger', u'want', u'to', u'stay', u'the', u'hell', u'out', u'of', u'the', u'Middle', u'East', u'.', u'Noth', u'i', u'go', u'to', u'stop', u'that', u'sh.thole', u'of', u'the', u'world', u'from', u'tear', u'each', u'other', u\"'\", u'throat', u'out', u'.', u'After', u'all', u',', u'they', u'have', u'been', u'do', u'it', u'successful', u'for', u'centur', u'.', u'Then']\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that this approach has obvious limitations. So, let's rely on a purpose-built stemmer. These rely in part on dictionaries. Note subtle differences between the two possible stemmers:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lancaster = nltk.LancasterStemmer()\n",
      "porter = nltk.PorterStemmer()\n",
      "stems = [lancaster.stem(t) for t in tokens]  # replace lancaster with porter here\n",
      "print stems[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'no', u'greens-win', u',', u'many', u'of', u'us', u'right', u'wing', u'want', u'to', u'stay', u'the', u'hel', u'out', u'of', u'the', u'middl', u'east', u'.', u'noth', u'is', u'going', u'to', u'stop', u'that', u'sh.thole', u'of', u'the', u'world', u'from', u'tear', u'each', u'oth', u\"'\", u'throats', u'out', u'.', u'aft', u'al', u',', u'they', u'hav', u'been', u'doing', u'it', u'success', u'for', u'century', u'.', u'then', u'you', u'bet', u'start', u'educ', u'yo', u'hard', u'right', u'mat', u'about', u'green', u\"'\", u'renew', u'energy', u'.', u'soon', u'we', u'end', u'our', u'addict', u'to', u'arab', u'oil', u'and', u'middl', u'eastern', u'export', u',', u'lik', u'liv', u'anim', u',', u'the', u'soon', u'we', u'can', u'dist', u'ourselv', u'.', u'i', u'am', u'not', u'say', u'that', u'al', u'muslim', u'ar', u'lik', u'thi', u'.', u'howev']\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that both stemmers handle some things rather poorly. The main reason for this is that they are not aware of the *word class* of any particular word: *nothing* is a noun, and nouns ending in *ing* should not have *ing* removed by the stemmer (swing, bling, ring...). Later in the course, we'll start annotating corpora with grammatical information. This improves the accuracy of stemmers a lot.\n",
      "\n",
      "> Note: stemming is not *always* the best thing to do: though *thing* is the stem of *things*, things has a unique meaning, as in *things will improve*. If we are interested in vague language, we may not want to collapse things --> thing."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Keywording: 'the aboutness of a text'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Keywording is the process of genererating a list of words that are unusually frequent in the corpus of interest. To do it, you need a *reference corpus*, or at least a *reference wordlist* to which your *target corpus* can be compared. Often, *reference corpora* take the form of very large collections of language drawn from a variety of spoken and written sources.\n",
      "\n",
      "Keywording is what generates word-clouds beside online news stories, blog posts, and the like. In combination with speech-to-text, it's used in Oxford Uni's [Spindle Project](http://openspires.oucs.ox.ac.uk/spindle/) to automatically archive recorded lectures with useful tags.\n",
      "\n",
      "In fact, the keywording part of the Spindle Project is written in Python, and open source.\n",
      "\n",
      "Spindle has sensible defaults for keyword calculation. Let's download their code and use it to generate keywords from our corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      " # download spindle from github\n",
      "!wget https://github.com/sgrau/spindle-code/archive/master.zip\n",
      "!unzip master.zip # unzip it\n",
      "!rm master.zip # remove the zip file\n",
      "# put the keyworder directory in python's path, so we can call it easily\n",
      "sys.path.insert(0, 'spindle-code-master/keywords')\n",
      "# import the function\n",
      "from keywords import keywords_and_ngrams # import keywords function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this tool works with raw text, not tokens!\n",
      "keywords_and_ngrams(raw.encode(\"UTF-8\"), nBigrams = 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 73,
       "text": [
        "([('isis', 932.9607164734134),\n",
        "  ('terrorist', 680.0627712571667),\n",
        "  ('muslim', 568.1485501205741),\n",
        "  ('muslims', 563.6329400959509),\n",
        "  ('genocide', 403.87716664798927),\n",
        "  ('iraq', 368.89945131193144),\n",
        "  ('attacks', 329.7408713036507),\n",
        "  ('isil', 319.17077142511516),\n",
        "  ('australian', 259.15906336931),\n",
        "  ('moslems', 227.71717950005285),\n",
        "  ('middle', 215.35707761243663),\n",
        "  ('military', 209.018280568296),\n",
        "  ('islamic', 207.24581015696876),\n",
        "  ('syria', 180.6355631540565),\n",
        "  ('shale', 171.86118461352353),\n",
        "  ('australians', 159.9616861419146),\n",
        "  ('uncle', 158.41771696231382),\n",
        "  ('east', 145.0887037296805),\n",
        "  ('arab', 132.27103092059374),\n",
        "  ('australia', 127.06952540132508),\n",
        "  ('western', 122.79035271023501),\n",
        "  ('war', 119.27467665652489),\n",
        "  ('oil', 115.0476986960435),\n",
        "  ('anzac', 110.4821901086937),\n",
        "  ('abbott', 104.58287134344383),\n",
        "  ('islam', 104.58287134344383),\n",
        "  ('jihad', 100.05977238095869),\n",
        "  ('foreign', 99.3697059634182),\n",
        "  ('obama', 98.20639120772773),\n",
        "  ('terror', 97.86297888925839),\n",
        "  ('moslem', 94.45119841477964),\n",
        "  ('terrorism', 89.17225935572746),\n",
        "  ('majority', 87.9223632228362),\n",
        "  ('jihadists', 85.93059230676177),\n",
        "  ('carried', 80.43381339333962),\n",
        "  ('kurds', 76.4045278832614),\n",
        "  ('troops', 75.77692827109284),\n",
        "  ('syrian', 73.72627700497003),\n",
        "  ('wiki', 73.6547934057958),\n",
        "  ('karnal', 73.6547934057958),\n",
        "  ('assad', 67.91748852952678),\n",
        "  ('vietnam', 67.51640653595446),\n",
        "  ('three', 67.3920497947377),\n",
        "  ('condemn', 64.73920211041637),\n",
        "  ('saudis', 64.66607825143228),\n",
        "  ('asio', 61.37899450482983),\n",
        "  ('taliban', 61.37899450482983),\n",
        "  ('jihadist', 61.37899450482983),\n",
        "  ('rooty', 61.37899450482983),\n",
        "  ('invasion', 60.96721684366244),\n",
        "  ('fighting', 59.8580697035693),\n",
        "  ('threat', 59.20256212374142),\n",
        "  ('killing', 58.0443397501277),\n",
        "  ('passports', 57.732616848767606),\n",
        "  ('ira', 55.976583578202884),\n",
        "  ('enemies', 53.433667761227994),\n",
        "  ('security', 50.82137859219669),\n",
        "  ('ideology', 49.36097832720489),\n",
        "  ('drones', 49.10319560386387),\n",
        "  ('karmal', 49.10319560386387),\n",
        "  ('lefties', 49.10319560386387),\n",
        "  ('beheading', 49.10319560386387),\n",
        "  ('sunni', 49.10319560386387),\n",
        "  ('labor', 49.10319560386387),\n",
        "  ('religious', 47.444768658753205),\n",
        "  ('wars', 47.33576964759126),\n",
        "  ('countries', 46.65747986034412),\n",
        "  ('substantiate', 46.24673273695559),\n",
        "  ('committed', 45.51612742983676),\n",
        "  ('backed', 44.93632445575847),\n",
        "  ('sparky', 44.1034949482507),\n",
        "  ('innocents', 44.1034949482507),\n",
        "  ('history', 43.16715757964919),\n",
        "  ('religion', 42.57480085696655),\n",
        "  ('jets', 42.326805253672525),\n",
        "  ('asylum', 42.08790416596874),\n",
        "  ('fight', 41.82857731924522),\n",
        "  ('iran', 41.710084684781535),\n",
        "  ('leaders', 41.19227166942919),\n",
        "  ('iraqis', 40.833762483396),\n",
        "  ('actions', 40.59105996115727),\n",
        "  ('groups', 39.884723238245044),\n",
        "  ('snake', 39.36067154124294),\n",
        "  ('proven', 38.76806342214171),\n",
        "  ('saddam', 38.41886335529832),\n",
        "  ('passport', 38.204198079398964),\n",
        "  ('masse', 38.030135033979576),\n",
        "  ('unify', 38.030135033979576),\n",
        "  ('aussies', 38.030135033979576),\n",
        "  ('posted', 37.93220871605723),\n",
        "  ('uncles', 37.87026023538432),\n",
        "  ('greens', 37.406641701391884),\n",
        "  ('attack', 37.40552809603401),\n",
        "  ('send', 37.37425645173262),\n",
        "  ('innocent', 37.25255067358685),\n",
        "  ('fbi', 36.8273967028979),\n",
        "  ('genocides', 36.8273967028979),\n",
        "  ('militias', 36.8273967028979),\n",
        "  ('simpletons', 36.8273967028979),\n",
        "  ('islamism', 36.8273967028979)],\n",
        " [])"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Success! We have keywords.\n",
      "\n",
      "> Keep in mind, the BNC reference corpus was created before ISIS and ISIL existed. *Moslem/moslems* is a dispreferred spelling of Muslim, used more frequently in anti-Islamic discourse. Also, it's unlikely that a transcriber of the spoken BNC would choose the Moslem spelling. *Having an inappropriate reference corpus is a common methodological problem in discourse analytic work*.\n",
      "\n",
      "Our keywords would perhaps be better if they were stemmed. That shouldn't be too hard for us:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "keywords_and_ngrams(stems, nBigrams = 0) # this will use our corpus of stems, as defined earlier."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "([(u'hav', 2577.128831495924),\n",
        "  (u'thi', 1346.8220212886622),\n",
        "  (u'wil', 1333.184820707811),\n",
        "  (u'muslim', 1318.1262855818427),\n",
        "  (u'ter', 1204.7477617358181),\n",
        "  (u'ther', 855.6401335082292),\n",
        "  (u'lik', 840.5484129999948),\n",
        "  (u'peopl', 734.2203696777686),\n",
        "  (u'iraq', 727.6153814041819),\n",
        "  (u'islam', 647.6304008281886),\n",
        "  (u'middl', 631.1764263038706),\n",
        "  (u'wer', 625.2041001855472),\n",
        "  (u'genocid', 624.4073925142818),\n",
        "  (u'wel', 617.3180337879181),\n",
        "  (u'aust', 602.5213700772096),\n",
        "  (u'tak', 550.5064069436636),\n",
        "  (u'stat', 543.3411532971542),\n",
        "  (u'oth', 540.1547245047125),\n",
        "  (u'moslem', 536.3460119847367),\n",
        "  (u'thes', 530.482580811814),\n",
        "  (u'mor', 520.9543686791584),\n",
        "  (u'believ', 516.3368822714253),\n",
        "  (u'becaus', 511.90087323079086),\n",
        "  (u'syr', 492.3212133285684),\n",
        "  (u'mak', 476.6442716480351),\n",
        "  (u'unc', 468.30554438571136),\n",
        "  (u'milit', 432.2820409714259),\n",
        "  (u'austral', 432.2820409714259),\n",
        "  (u'thos', 422.99395106825125),\n",
        "  (u'kil', 411.14410256099256),\n",
        "  (u'nee', 397.3783746207032),\n",
        "  (u'wher', 394.5008942108771),\n",
        "  (u'attack', 393.4166511768609),\n",
        "  (u'som', 391.9806048827137),\n",
        "  (u'isil', 360.2350341428549),\n",
        "  (u'forc', 348.22719967142643),\n",
        "  (u'cal', 344.1255028768332),\n",
        "  (u'govern', 341.27192037524424),\n",
        "  (u'sint', 336.2193651999979),\n",
        "  (u'jihad', 321.53345205020673),\n",
        "  (u'war', 316.080103835816),\n",
        "  (u'maj', 304.721385011913),\n",
        "  (u'nat', 301.7553830727658),\n",
        "  (u'doe', 298.71453868312256),\n",
        "  (u'nev', 280.3018284600244),\n",
        "  (u'involv', 274.096068373995),\n",
        "  (u'giv', 269.3659268255907),\n",
        "  (u'liv', 258.1935472756536),\n",
        "  (u'hap', 249.55965256674102),\n",
        "  (u'stil', 245.84094149462192),\n",
        "  (u'fight', 243.96202124015622),\n",
        "  (u'themselv', 240.15668942856993),\n",
        "  (u'tel', 230.91274264042718),\n",
        "  (u'nam', 228.14885495714145),\n",
        "  (u'east', 225.5307238898069),\n",
        "  (u'noth', 223.4570651522528),\n",
        "  (u'bas', 220.70836540394964),\n",
        "  (u'arab', 211.16429756619607),\n",
        "  (u'hist', 210.63817463653942),\n",
        "  (u'tim', 207.07498763796195),\n",
        "  (u'howev', 196.41400065687736),\n",
        "  (u'anyth', 195.2986676269421),\n",
        "  (u'alway', 192.12535154285595),\n",
        "  (u'someon', 192.12535154285595),\n",
        "  (u'viol', 192.12535154285595),\n",
        "  (u'plac', 192.12535154285595),\n",
        "  (u'karn', 192.12535154285595),\n",
        "  (u'anzac', 192.12535154285595),\n",
        "  (u'polit', 187.2396551888753),\n",
        "  (u'someth', 187.2396551888753),\n",
        "  (u'littl', 184.5238811855611),\n",
        "  (u'ideolog', 180.11751707142744),\n",
        "  (u'becom', 180.11751707142744),\n",
        "  (u'commun', 179.57728455039378),\n",
        "  (u'cours', 172.64112822176426),\n",
        "  (u'solv', 172.64112822176426),\n",
        "  (u'shal', 167.8122465867092),\n",
        "  (u'leav', 167.8122465867092),\n",
        "  (u'relig', 160.76672602510337),\n",
        "  (u'commit', 157.73681003077445),\n",
        "  (u'diff', 155.70346463635417),\n",
        "  (u'und', 155.68108241023776),\n",
        "  (u'country', 154.0725958138106),\n",
        "  (u'def', 153.1826896975527),\n",
        "  (u'aft', 153.1826896975527),\n",
        "  (u'troop', 150.52261587721415),\n",
        "  (u'pol', 150.43690767017821),\n",
        "  (u'numb', 150.43690767017821),\n",
        "  (u'behead', 148.90187042228666),\n",
        "  (u'loc', 146.23370688995993),\n",
        "  (u'tru', 144.74608564731926),\n",
        "  (u'afgh', 144.09401365714197),\n",
        "  (u'religy', 144.09401365714197),\n",
        "  (u'innoc', 144.09401365714197),\n",
        "  (u'issu', 144.09401365714197),\n",
        "  (u'fac', 139.30803837988563),\n",
        "  (u'hom', 137.0480341869975),\n",
        "  (u'valu', 137.0480341869975),\n",
        "  (u'condemn', 136.30021817568206),\n",
        "  (u'min', 133.5491289888459)],\n",
        " [])"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Damn. Keywording with stems actually revealed a list of incorrect stems.\n",
      "\n",
      "What re really need to do is improve our stemmer, and then come back and try again."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A return to stemming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Keywords and ngram searches actually work by comparing a corpus to a reference corpus. In our case, we have been using a dictionary of words in the 100 million word *British National Corpus*. We could use this same dictionary to make sure our stemmer does not create non-words when it stems.\n",
      "\n",
      "First, let's get a list of common words in the BNC from the *pickle* provided by SPINDLE (*pickle* is a kind of list compression)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "import os\n",
      "bncwordlist = pickle.load(open('spindle-code-master/keywords/bnc.p', 'rb')) # unpack the pickled list\n",
      "bnc_commonwords = [] # empty list\n",
      "for word in bncwordlist:\n",
      "    getval = bncwordlist[word] # find out number of occurrences of word\n",
      "    if getval > 20: # if more than 20\n",
      "        bnc_commonwords.append(word) # add to common word list\n",
      "print bnc_commonwords[:200] # what are our results?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['raining', 'woods', 'hanging', 'sevens', 'screaming', 'receiving', 'wooden', 'elgar', 'feasibility', 'errors', 'cooking', 'designing', 'china', 'kids', 'climbed', 'controversy', 'millimetres', 'golden', 'projection', 'music', 'populations', 'intake', 'morally', 'locked', 'locker', 'deputy', 'brochure', 'cookery', 'absolute', 'travel', 'playback', 'dinosaurs', 'wrong', 'promotion', 'opted', 'calculations', 'welcomed', 'aargh', 'playhouse', 'fir', 'fit', 'bringing', 'fif', 'wales', 'vouchers', 'effects', 'sixteen', 'barton', 'arrow', 'telescope', 'mason', 'encourage', 'adapt', 'estimate', 'chlorine', 'silent', 'husbands', 'disturbed', 'breed', 'specially', 'households', 'olds', 'service', 'needed', 'master', 'hiya', 'genesis', 'rewards', 'positively', 'regulator', 'idle', 'friend', 'feeling', 'spectrum', 'arousal', 'dozen', 'affairs', 'achievement', 'committing', 'mouth', 'bradford', 'singer', 'tech', 'thunder', 'scream', 'padded', 'tempted', 'cheaply', 'charges', 'rick', 'rich', 'rice', 'photographers', 'plate', 'plato', 'altogether', 'jaguar', 'cunnane', 'nicely', 'patch', 'clarified', 'sensitivity', 'lots', 'targets', 'letting', 'extend', 'nature', 'fruits', 'extent', 'heating', 'gypsies', 'salad', 'righteousness', 'union', 'fro', 'bothers', 'fry', 'spit', 'dave', 'doubts', 'spin', 'professionally', 'employ', 'eighteen', 'conditioner', 'memorial', 'ignored', 'split', 'boiled', 'qualifications', 'marched', 'boiler', 'supper', 'academic', 'corporate', 'plaque', 'criticism', 'appropriately', 'favours', 'fairer', 'ham', 'advantages', 'hat', 'hav', 'knock', 'elders', 'survival', 'credit', 'shadow', 'replace', 'semi', 'alice', 'beneficial', 'crowd', 'crown', 'bottom', 'binder', 'benches', 'maxwell', 'honeymoon', 'administer', 'beings', 'shoots', 'fabric', 'raped', 'humorous', 'waistcoat', 'safeguard', 'clause', 'complications', 'smashed', 'passenger', 'disgrace', 'mirror', 'females', 'hydrogen', 'triangles', 'role', 'roll', 'intend', 'transported', 'devon', 'intent', 'smelling', 'variable', 'filing', 'gown', 'chain', 'chair', 'ballet', 'circumstances', 'choice', 'embark', 'stays', 'exact', 'minute', 'cooks', 'meadow', 'chopping', 'shirts']\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, this gives us a list of any word appearing more than twenty times in the BNC. We could build this function into our stemmer:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now, let's use this as the dict for our stemmer\n",
      "# The third variable sets a default threshold, but also allows us to enter one.\n",
      "def newstemmer(words, stemmer, threshold = 20):\n",
      "    \"\"\"A stemmer that uses Lancaster/porter stemmer plus a dictionary.\"\"\"\n",
      "    import pickle\n",
      "    import os\n",
      "    import nltk\n",
      "    bncwordlist = pickle.load(open('spindle-code-master/keywords/bnc.p', 'rb'))\n",
      "    bnc_commonwords = {k for (k,v) in bncwordlist.iteritems() if v > threshold}\n",
      "    # if words is a raw string, tokenise it\n",
      "    if type(words) == unicode or type(words) == string:\n",
      "        tokens = nltk.word_tokenize(words)\n",
      "    # or, if list of tokens, duplicate the list\n",
      "    else:\n",
      "        tokens = words\n",
      "    if stemmer == 'Lancaster':\n",
      "        stemmertouse = nltk.LancasterStemmer()\n",
      "    if stemmer == 'Porter':\n",
      "        stemmertouse = nltk.PorterStemmer()\n",
      "    stems = []\n",
      "    for w in tokens:\n",
      "        stem = stemmertouse.stem(w)\n",
      "        if stem in bnc_commonwords:\n",
      "            stems.append(stem)\n",
      "        else:\n",
      "            stems.append(w)\n",
      "    return stems"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we can fiddle with the stemmer and BNC frequency to get different keyword lists."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try raising the threshold if there are still bad spellings!\n",
      "stemmed = newstemmer(raw, 'Lancaster', 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "keys = keywords_and_ngrams(stemmed)\n",
      "pprint.pprint(keys[0]) # only keywords\n",
      "pprint.pprint(keys[1]) # only n-grams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'hav', 2577.128831495924),\n",
        " (u'thi', 1346.8220212886622),\n",
        " (u'muslim', 1318.1262855818427),\n",
        " (u'isis', 1188.7756126714212),\n",
        " (u'ther', 855.6401335082292),\n",
        " (u'iraq', 727.6153814041819),\n",
        " (u'terrorist', 674.2812420368347),\n",
        " (u'islam', 647.6304008281886),\n",
        " (u'wer', 625.2041001855472),\n",
        " (u'wel', 617.3180337879181),\n",
        " (u'genocide', 560.0414672830459),\n",
        " (u'stat', 543.3411532971542),\n",
        " (u'oth', 540.1547245047125),\n",
        " (u'mor', 520.9543686791584),\n",
        " (u'nee', 397.3783746207032),\n",
        " (u'attack', 393.4166511768609),\n",
        " (u'som', 391.9806048827137),\n",
        " (u'moslems', 387.4574685856344),\n",
        " (u'isil', 360.2350341428549),\n",
        " (u'cal', 344.1255028768332),\n",
        " (u'syria', 344.1255028768332),\n",
        " (u'govern', 341.27192037524424),\n",
        " (u'war', 316.080103835816),\n",
        " (u'nat', 301.7553830727658),\n",
        " (u'doe', 298.71453868312256),\n",
        " (u'nev', 280.3018284600244),\n",
        " (u'australian', 269.72544133182345),\n",
        " (u'fight', 243.96202124015622),\n",
        " (u'australia', 231.03344786931623),\n",
        " (u'tel', 230.91274264042718),\n",
        " (u'east', 225.5307238898069),\n",
        " (u'noth', 223.4570651522528),\n",
        " (u'arab', 211.16429756619607),\n",
        " (u'military', 208.68038229454555),\n",
        " (u'tim', 207.07498763796195),\n",
        " (u'middle', 201.6888771512355),\n",
        " (u'uncle', 197.16876439089108),\n",
        " (u'karnal', 192.12535154285595),\n",
        " (u'shale', 168.10968259999896),\n",
        " (u'australians', 166.66706966232692),\n",
        " (u'terrorism', 159.971011400232),\n",
        " (u'commit', 157.73681003077445),\n",
        " (u'anzac', 156.10184812857045),\n",
        " (u'diff', 155.70346463635417),\n",
        " (u'und', 155.68108241023776),\n",
        " (u'country', 154.0725958138106),\n",
        " (u'def', 153.1826896975527),\n",
        " (u'aft', 153.1826896975527),\n",
        " (u'troop', 150.52261587721415),\n",
        " (u'tru', 144.74608564731926),\n",
        " (u'moslem', 137.57140952939218),\n",
        " (u'condemn', 136.30021817568206),\n",
        " (u'min', 133.5491289888459),\n",
        " (u'carry', 132.89256476846742),\n",
        " (u'jihad', 132.6206452029535),\n",
        " (u'poss', 129.36265969385093),\n",
        " (u'pow', 129.36265969385093),\n",
        " (u'passport', 125.31692677904687),\n",
        " (u'western', 123.7713623908227),\n",
        " (u'abbott', 123.05203008581161),\n",
        " (u'bef', 122.13894767635674),\n",
        " (u'quest', 120.12246971571872),\n",
        " (u'vietnam', 117.13612215950897),\n",
        " (u'mat', 116.85351626216469),\n",
        " (u'saddam', 116.44737573638093),\n",
        " (u'terror', 113.98661865815663),\n",
        " (u'assad', 113.38129480154524),\n",
        " (u'oil', 108.38598292206339),\n",
        " (u'jihadists', 108.07051024285647),\n",
        " (u'obama', 108.07051024285647),\n",
        " (u'foreign', 107.51020344575288),\n",
        " (u'sid', 107.17474674779933),\n",
        " (u'hel', 107.07225195503892),\n",
        " (u'sur', 107.07225195503892),\n",
        " (u'fig', 103.80230975756946),\n",
        " (u'ideology', 102.15622044861591),\n",
        " (u'civil', 100.66581084937829),\n",
        " (u'cur', 99.0462496008416),\n",
        " (u'afghanistan', 98.93798646689051),\n",
        " (u'wiki', 96.06267577142798),\n",
        " (u'support', 94.33322574218278),\n",
        " (u'majority', 93.69095781080296),\n",
        " (u'bomb', 93.03817413351861),\n",
        " (u'enemy', 86.8655256478213),\n",
        " (u'prob', 86.4920119937804),\n",
        " (u'kurds', 86.06451575855121),\n",
        " (u'fin', 85.5109733718177),\n",
        " (u'gen', 84.5471354855585),\n",
        " (u'hum', 82.80950999320149),\n",
        " (u'wag', 80.59934421269136),\n",
        " (u'situ', 79.4477005927586),\n",
        " (u'three', 78.13191912919723),\n",
        " (u'saudis', 74.53001817484869),\n",
        " (u'mad', 73.00447194184991),\n",
        " (u'group', 72.05364342612089),\n",
        " (u'asio', 72.04700682857099),\n",
        " (u'fbi', 72.04700682857099),\n",
        " (u'taliban', 72.04700682857099),\n",
        " (u'jihadist', 72.04700682857099),\n",
        " (u'post', 72.04040753580341)]\n",
        "[((u'middle', u'east'), 49),\n",
        " ((u'terrorist', u'attack'), 31),\n",
        " ((u'terrorist', u'group'), 12),\n",
        " ((u'shale', u'oil'), 10),\n",
        " ((u'foreign', u'policy'), 8),\n",
        " ((u'commit', u'genocide'), 8),\n",
        " ((u'terror', u'attack'), 7),\n",
        " ((u'islam', u'stat'), 7),\n",
        " ((u'thi', u'tim'), 6),\n",
        " ((u'muslim', u'terrorist'), 6),\n",
        " ((u'civil', u'war'), 6),\n",
        " ((u'air', u'strikes'), 6),\n",
        " ((u'wholesale', u'genocide'), 5),\n",
        " ((u'oil', u'wel'), 5),\n",
        " ((u'muslim', u'country'), 5),\n",
        " ((u'australian', u'govern'), 5),\n",
        " ((u'western', u'values'), 4),\n",
        " ((u'wag', u'war'), 4),\n",
        " ((u'thi', u'country'), 4),\n",
        " ((u'support', u'isil'), 4),\n",
        " ((u'storm', u'troop'), 4),\n",
        " ((u'political', u'lead'), 4),\n",
        " ((u'muslim', u'group'), 4),\n",
        " ((u'islam', u'terrorism'), 4),\n",
        " ((u'hav', u'nev'), 4)]\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Collocation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> *You shall know a word by the company it keeps.* - J.R. Firth, 1957\n",
      "\n",
      "Collocation is a very common area of interest in corpus linguistics. Words pattern together in both expected and unexpected ways. In some contexts, *drug* and *medication* are synonymous, but it would be very rare to hear about *illicit* or *street medication*. Similarly, doctors are unlikely to perscribe the *correct* or *appropriate drug*.\n",
      "\n",
      "This kind of information may be useful to lexicographers, discourse analysts, or advanced language learners.\n",
      "\n",
      "In NLTK, collocation works from ordered lists of tokens. Let's put out tokenised sents into a single, huge list of tokens:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allwords = []\n",
      "for sent in tokenized_sents:\n",
      "    for word in sent:\n",
      "        allwords.append(word)\n",
      "print allwords[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'No', u'Greens-win', u',', u'many', u'of', u'us', u'right', u'wingers', u'want', u'to', u'stay', u'the', u'hell', u'out', u'of', u'the', u'Middle', u'East', u'.', u'Nothing']\n"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's feed these to an NLTK function for measuring collocations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.collocations import *\n",
      "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
      "finder = BigramCollocationFinder.from_words(allwords)\n",
      "\n",
      "sorted(finder.nbest(bigram_measures.raw_freq, 30))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "[(u',', u'I'),\n",
        " (u',', u'and'),\n",
        " (u',', u'but'),\n",
        " (u',', u'it'),\n",
        " (u',', u'the'),\n",
        " (u'.', u'And'),\n",
        " (u'.', u'I'),\n",
        " (u'.', u'If'),\n",
        " (u'.', u'It'),\n",
        " (u'.', u'The'),\n",
        " (u'.', u'You'),\n",
        " (u'?', u'?'),\n",
        " (u'Middle', u'East'),\n",
        " (u'and', u'the'),\n",
        " (u'do', u\"n't\"),\n",
        " (u'for', u'the'),\n",
        " (u'have', u'been'),\n",
        " (u'in', u'the'),\n",
        " (u'is', u'a'),\n",
        " (u'it', u'is'),\n",
        " (u'of', u'the'),\n",
        " (u'on', u'the'),\n",
        " (u'out', u'of'),\n",
        " (u'that', u'the'),\n",
        " (u'the', u'Middle'),\n",
        " (u'the', u'US'),\n",
        " (u'the', u'world'),\n",
        " (u'they', u'are'),\n",
        " (u'to', u'be'),\n",
        " (u'to', u'the')]"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, that tells us a little: we can see that terrorists, Muslims and the Middle East are commonly collocating in the text. At present, we are only looking for immediately adjacent words. So, let's expand out search to a window of *five words either side*\n",
      "\n",
      "window size specifies the distance\n",
      "at which two tokens can still be considered collocates\n",
      "finder = BigramCollocationFinder.from_words(allwords, window_size=5)\n",
      "sorted(finder.nbest(bigram_measures.raw_freq, 30))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have the appearance of very common words! Let's use NLTK's stopwords list to remove entries containing these:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "finder = BigramCollocationFinder.from_words(allwords, window_size=5)\n",
      "ignored_words = nltk.corpus.stopwords.words('english')\n",
      "finder.apply_word_filter(lambda w: len(w) < 2 or w.lower() in ignored_words)\n",
      "finder.apply_freq_filter(2)\n",
      "sorted(finder.nbest(bigram_measures.raw_freq, 30))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "[(u'...', u'..'),\n",
        " (u'...', u'...'),\n",
        " (u'2001', u'carried'),\n",
        " (u'Iraq', u'Syria'),\n",
        " (u'Middle', u'East'),\n",
        " (u'Syria', u'Iraq'),\n",
        " (u'``', u\"''\"),\n",
        " (u'``', u'...'),\n",
        " (u'around', u'world'),\n",
        " (u'attacks', u'around'),\n",
        " (u'attacks', u'since'),\n",
        " (u'attacks', u'world'),\n",
        " (u'ca', u\"n't\"),\n",
        " (u'carried', u'non-Muslims'),\n",
        " (u'etc', u'etc'),\n",
        " (u'majority', u'around'),\n",
        " (u'majority', u'attacks'),\n",
        " (u'majority', u'terrorist'),\n",
        " (u'middle', u'east'),\n",
        " (u\"n't\", u'know'),\n",
        " (u\"n't\", u'think'),\n",
        " (u'shale', u'oil'),\n",
        " (u'since', u'2001'),\n",
        " (u'spirit', u'Anzac'),\n",
        " (u'terrorist', u'around'),\n",
        " (u'terrorist', u'attacks'),\n",
        " (u'terrorist', u'groups'),\n",
        " (u'terrorist', u'world'),\n",
        " (u'world', u'2001'),\n",
        " (u'world', u'since')]"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There! Now we have some interesting collocates. Finally, let's remove punctuation-only entries, or entries that are *n't*, as this is caused by different tokenisers:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "finder = BigramCollocationFinder.from_words(allwords, window_size=5)\n",
      "ignored_words = nltk.corpus.stopwords.words('english')\n",
      "# anything containing letter or number\n",
      "regex = r'[A-Za-z0-9]'\n",
      "# the n't token\n",
      "nonot = r'n\\'t'\n",
      "# lots of conditions!\n",
      "finder.apply_word_filter(lambda w: len(w) < 2 or w.lower() in ignored_words or not re.match(regex, w) or re.match(nonot, w))\n",
      "finder.apply_freq_filter(2)\n",
      "sorted(finder.nbest(bigram_measures.raw_freq, 30))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 83,
       "text": [
        "[(u'2001', u'carried'),\n",
        " (u'Cold', u'War'),\n",
        " (u'Iraq', u'Afghanistan'),\n",
        " (u'Iraq', u'Syria'),\n",
        " (u'Middle', u'East'),\n",
        " (u'Syria', u'Iraq'),\n",
        " (u'around', u'2001'),\n",
        " (u'around', u'since'),\n",
        " (u'around', u'world'),\n",
        " (u'attacks', u'around'),\n",
        " (u'attacks', u'carried'),\n",
        " (u'attacks', u'since'),\n",
        " (u'attacks', u'world'),\n",
        " (u'carried', u'non-Muslims'),\n",
        " (u'etc', u'etc'),\n",
        " (u'foreign', u'policy'),\n",
        " (u'majority', u'around'),\n",
        " (u'majority', u'attacks'),\n",
        " (u'majority', u'terrorist'),\n",
        " (u'middle', u'east'),\n",
        " (u'shale', u'oil'),\n",
        " (u'since', u'2001'),\n",
        " (u'since', u'carried'),\n",
        " (u'spirit', u'Anzac'),\n",
        " (u'terrorist', u'around'),\n",
        " (u'terrorist', u'attacks'),\n",
        " (u'terrorist', u'groups'),\n",
        " (u'terrorist', u'world'),\n",
        " (u'world', u'2001'),\n",
        " (u'world', u'since')]"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can get a lot more info on collocation at the [NLTK homepage](http://www.nltk.org/howto/collocations.html)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Clustering/n-grams"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clustering is the task of finding words that are commonly **immediately** adjacent (as opposed to collocates, which may just be nearby). This is also often called n-grams: bigrams are two tokens that appear together, trigrams are three, etc.\n",
      "\n",
      "Clusters/n-grams have a spooky ability to tell us what a text is about.\n",
      "\n",
      "We can use *Spindle* for bigram searching as well:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# an argument here to stop keywords from being produced.\n",
      "keywords_and_ngrams(raw.encode(\"UTF-8\"), nKeywords=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 85,
       "text": [
        "([],\n",
        " [(('middle', 'east'), 34),\n",
        "  (('terrorist', 'attacks'), 25),\n",
        "  (('shale', 'oil'), 10),\n",
        "  (('terrorist', 'groups'), 9),\n",
        "  (('muslim', 'terrorist'), 6),\n",
        "  (('wholesale', 'genocide'), 5),\n",
        "  (('terror', 'attacks'), 5),\n",
        "  (('committing', 'genocide'), 5),\n",
        "  (('air', 'strikes'), 5),\n",
        "  (('terrorist', 'attack'), 4),\n",
        "  (('islamic', 'terrorism'), 4),\n",
        "  (('fighter', 'jets'), 4),\n",
        "  (('australian', 'government'), 4),\n",
        "  (('syrian', 'civil'), 3),\n",
        "  (('political', 'leaders'), 3)])"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There's also a method for n-gram production in NLTK. We can use this to understand how n-gramming works.\n",
      "\n",
      "Below, we get lists of any ten adjacent tokens:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.util import ngrams\n",
      "sentence = 'give a man a fish and you feed him for a day; teach a man to fish and you feed him for a lifetime'\n",
      "n = 10\n",
      "tengrams = ngrams(sentence.split(), n)\n",
      "for gram in tengrams:\n",
      "  print gram"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('give', 'a', 'man', 'a', 'fish', 'and', 'you', 'feed', 'him', 'for')\n",
        "('a', 'man', 'a', 'fish', 'and', 'you', 'feed', 'him', 'for', 'a')\n",
        "('man', 'a', 'fish', 'and', 'you', 'feed', 'him', 'for', 'a', 'day;')\n",
        "('a', 'fish', 'and', 'you', 'feed', 'him', 'for', 'a', 'day;', 'teach')\n",
        "('fish', 'and', 'you', 'feed', 'him', 'for', 'a', 'day;', 'teach', 'a')\n",
        "('and', 'you', 'feed', 'him', 'for', 'a', 'day;', 'teach', 'a', 'man')\n",
        "('you', 'feed', 'him', 'for', 'a', 'day;', 'teach', 'a', 'man', 'to')\n",
        "('feed', 'him', 'for', 'a', 'day;', 'teach', 'a', 'man', 'to', 'fish')\n",
        "('him', 'for', 'a', 'day;', 'teach', 'a', 'man', 'to', 'fish', 'and')\n",
        "('for', 'a', 'day;', 'teach', 'a', 'man', 'to', 'fish', 'and', 'you')\n",
        "('a', 'day;', 'teach', 'a', 'man', 'to', 'fish', 'and', 'you', 'feed')\n",
        "('day;', 'teach', 'a', 'man', 'to', 'fish', 'and', 'you', 'feed', 'him')\n",
        "('teach', 'a', 'man', 'to', 'fish', 'and', 'you', 'feed', 'him', 'for')\n",
        "('a', 'man', 'to', 'fish', 'and', 'you', 'feed', 'him', 'for', 'a')\n",
        "('man', 'to', 'fish', 'and', 'you', 'feed', 'him', 'for', 'a', 'lifetime')\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could now wrap this up in a function, and use it to locate any duplicated n-grams:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ngrammer(text, gramsize, threshold = 2):\n",
      "    \"\"\"Get any repeating ngram containing gramsize tokens\"\"\"\n",
      "    from collections import defaultdict\n",
      "    def list_duplicates(seq):\n",
      "        tally = defaultdict(list)\n",
      "        for i,item in enumerate(seq):\n",
      "            tally[item].append(i)\n",
      "        return ((len(locs),key) for key,locs in tally.items() \n",
      "               if len(locs) > threshold)\n",
      "    from nltk.util import ngrams\n",
      "    raw_grams = ngrams(text.split(), gramsize)\n",
      "    dupes = list_duplicates(raw_grams)\n",
      "    return sorted(dupes, reverse = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngrammer(raw, 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 88,
       "text": [
        "[(18, (u'the', u'Middle', u'East')),\n",
        " (18, (u'in', u'the', u'Middle')),\n",
        " (18, (u'carried', u'out', u'by')),\n",
        " (13, (u'out', u'by', u'non-Muslims.')),\n",
        " (13, (u'of', u'terrorist', u'attacks')),\n",
        " (12, (u'the', u'majority', u'of')),\n",
        " (12, (u'majority', u'of', u'terrorist')),\n",
        " (12, (u'around', u'the', u'world')),\n",
        " (10, (u'the', u'Middle', u'East.')),\n",
        " (10, (u'terrorist', u'attacks', u'around')),\n",
        " (10, (u'have', u'been', u'carried')),\n",
        " (10, (u'been', u'carried', u'out')),\n",
        " (10, (u'attacks', u'around', u'the')),\n",
        " (9, (u'the', u'world', u'since')),\n",
        " (9, (u'the', u'middle', u'east')),\n",
        " (8, (u'we', u'need', u'to')),\n",
        " (8, (u'to', u'do', u'with')),\n",
        " (8, (u'out', u'of', u'the')),\n",
        " (7, (u'world', u'since', u'2001')),\n",
        " (7, (u'to', u'deal', u'with')),\n",
        " (7, (u'the', u'rest', u'of')),\n",
        " (7, (u'since', u'2001', u'have')),\n",
        " (7, (u'of', u'the', u'world')),\n",
        " (7, (u'2001', u'have', u'been')),\n",
        " (6, (u'that', u'there', u'is')),\n",
        " (6, (u'rest', u'of', u'the')),\n",
        " (6, (u'nothing', u'to', u'do')),\n",
        " (6, (u'in', u'the', u'middle')),\n",
        " (6, (u'in', u'Iraq', u'and')),\n",
        " (6, (u'do', u'you', u'think')),\n",
        " (6, (u'Middle', u'East', u'is')),\n",
        " (5, (u'you', u'want', u'to')),\n",
        " (5, (u'you', u'are', u'a')),\n",
        " (5, (u'to', u'stop', u'the')),\n",
        " (5, (u'to', u'be', u'a')),\n",
        " (5, (u'terrorist', u'attacks', u'are')),\n",
        " (5, (u'spirit', u'of', u'Anzac')),\n",
        " (5, (u'on', u'the', u'ground')),\n",
        " (5, (u'need', u'to', u'be')),\n",
        " (5, (u'is', u'not', u'a')),\n",
        " (5, (u'in', u'Syria', u'and')),\n",
        " (5, (u'boots', u'on', u'the')),\n",
        " (5, (u'as', u'well', u'as')),\n",
        " (5, (u'are', u'carried', u'out')),\n",
        " (5, (u'This', u'is', u'a')),\n",
        " (5, (u'Syria', u'and', u'Iraq')),\n",
        " (5, (u'I', u\"don't\", u'think')),\n",
        " (4, (u'who', u'do', u'not')),\n",
        " (4, (u'what', u'do', u'you')),\n",
        " (4, (u'to', u'go', u'and')),\n",
        " (4, (u'there', u'is', u'no')),\n",
        " (4, (u'there', u'is', u'a')),\n",
        " (4, (u'the', u'spirit', u'of')),\n",
        " (4, (u'the', u'fact', u'that')),\n",
        " (4, (u'state', u'in', u'the')),\n",
        " (4, (u'spirit', u'of', u'Anzac.')),\n",
        " (4, (u'seem', u'to', u'be')),\n",
        " (4, (u'part', u'of', u'the')),\n",
        " (4, (u'of', u'the', u'US')),\n",
        " (4, (u'look', u'at', u'the')),\n",
        " (4, (u'is', u'that', u'the')),\n",
        " (4, (u'interested', u'in', u'the')),\n",
        " (4, (u'in', u'the', u'world')),\n",
        " (4, (u'have', u'a', u'plan')),\n",
        " (4, (u'go', u'in', u'and')),\n",
        " (4, (u'end', u'of', u'the')),\n",
        " (4, (u'but', u'I', u'do')),\n",
        " (4, (u'and', u'you', u'will')),\n",
        " (4, (u'and', u'George', u'Bush')),\n",
        " (4, (u'a', u'lot', u'of')),\n",
        " (4, (u'a', u'bit', u'of')),\n",
        " (4, (u'Tony', u'Blair', u'and')),\n",
        " (4, (u'The', u'days', u'of')),\n",
        " (4, (u'ISIS', u'is', u'not')),\n",
        " (4, (u'I', u'would', u'be')),\n",
        " (4, (u'I', u'also', u'believe')),\n",
        " (4, (u'Blair', u'and', u'George')),\n",
        " (4, (u'And', u'how', u'do')),\n",
        " (3, (u'you', u'will', u'see')),\n",
        " (3, (u'you', u'seem', u'to')),\n",
        " (3, (u'you', u'have', u'to')),\n",
        " (3, (u'you', u'are', u'saying')),\n",
        " (3, (u'would', u'like', u'to')),\n",
        " (3, (u'would', u'be', u'the')),\n",
        " (3, (u'would', u'be', u'a')),\n",
        " (3, (u'women', u'and', u'children')),\n",
        " (3, (u'will', u'not', u'be')),\n",
        " (3, (u'when', u'it', u'comes')),\n",
        " (3, (u'what', u'you', u'are')),\n",
        " (3, (u'we', u'now', u'have')),\n",
        " (3, (u'we', u'have', u'to')),\n",
        " (3, (u'want', u'to', u'know')),\n",
        " (3, (u'very', u'interested', u'in')),\n",
        " (3, (u'to', u'say', u'about')),\n",
        " (3, (u'to', u'look', u'at')),\n",
        " (3, (u'to', u'grow', u'strong')),\n",
        " (3, (u'to', u'do', u'something')),\n",
        " (3, (u'to', u'carry', u'out')),\n",
        " (3, (u'to', u'be', u'in')),\n",
        " (3, (u'to', u'Kingdom', u'Come')),\n",
        " (3, (u'thousands', u'of', u'people')),\n",
        " (3, (u'they', u'will', u'be')),\n",
        " (3, (u'they', u'are', u'not')),\n",
        " (3, (u'them', u'the', u'benefit')),\n",
        " (3, (u'the', u'wholesale', u'genocide')),\n",
        " (3, (u'the', u'situation', u'in')),\n",
        " (3, (u'the', u'number', u'of')),\n",
        " (3, (u'the', u'last', u'10')),\n",
        " (3, (u'the', u'invasion', u'of')),\n",
        " (3, (u'the', u'interests', u'of')),\n",
        " (3, (u'the', u'history', u'of')),\n",
        " (3, (u'the', u'ground', u'in')),\n",
        " (3, (u'the', u'benefit', u'of')),\n",
        " (3, (u'the', u'US', u'does')),\n",
        " (3, (u'that', u'you', u'have')),\n",
        " (3, (u'that', u'the', u'majority')),\n",
        " (3, (u'stand', u'by,', u'whilst')),\n",
        " (3, (u'some', u'sort', u'of')),\n",
        " (3, (u'right', u'now.', u'The')),\n",
        " (3, (u'over', u'and', u'over')),\n",
        " (3, (u'out', u'to', u'be')),\n",
        " (3, (u'our', u'political', u'leaders')),\n",
        " (3, (u'one', u'of', u'your')),\n",
        " (3, (u'on', u'their', u'own')),\n",
        " (3, (u'on', u'a', u'grand')),\n",
        " (3, (u'of', u'the', u'middle')),\n",
        " (3, (u'of', u'our', u'own')),\n",
        " (3, (u'of', u'Anzac', u'is')),\n",
        " (3, (u'not', u'have', u'a')),\n",
        " (3, (u'never', u'condemn', u'a')),\n",
        " (3, (u'needs', u'to', u'be')),\n",
        " (3, (u'need', u'to', u'get')),\n",
        " (3, (u'most', u'terrorist', u'attacks')),\n",
        " (3, (u'middle', u'east', u'and')),\n",
        " (3, (u'long', u'gone.', u'The')),\n",
        " (3, (u'keep', u'going', u'for')),\n",
        " (3, (u'it', u'comes', u'to')),\n",
        " (3, (u'is', u'left', u'to')),\n",
        " (3, (u'is', u'just', u'a')),\n",
        " (3, (u'is', u'going', u'to')),\n",
        " (3, (u'invasion', u'of', u'Iraq')),\n",
        " (3, (u'in', u'the', u'history')),\n",
        " (3, (u'in', u'the', u'desert.')),\n",
        " (3, (u'how', u'do', u'you')),\n",
        " (3, (u'grow', u'strong', u'and')),\n",
        " (3, (u'groups', u'that', u'are')),\n",
        " (3, (u'going', u'to', u'do')),\n",
        " (3, (u'food', u'and', u'water.')),\n",
        " (3, (u'exactly', u'the', u'same')),\n",
        " (3, (u'everything', u'that', u'is')),\n",
        " (3, (u'etc,', u'etc,', u'etc,')),\n",
        " (3, (u'does', u'not', u'have')),\n",
        " (3, (u'condemn', u'a', u'moslem')),\n",
        " (3, (u'by', u'non-Muslims.', u\"I've\")),\n",
        " (3, (u'because', u'of', u'the')),\n",
        " (3, (u'because', u'it', u'is')),\n",
        " (3, (u'attacks', u'are', u'carried')),\n",
        " (3, (u'are', u'long', u'gone.')),\n",
        " (3, (u'and', u'they', u'are')),\n",
        " (3, (u'and', u'that', u'the')),\n",
        " (3, (u'and', u'it', u'is')),\n",
        " (3, (u'also', u'believe', u'that')),\n",
        " (3, (u'all', u'the', u'other')),\n",
        " (3, (u'a', u'threat', u'to')),\n",
        " (3, (u'a', u'plan', u'for')),\n",
        " (3, (u'a', u'long', u'time')),\n",
        " (3, (u'a', u'load', u'of')),\n",
        " (3, (u'What', u'a', u'load')),\n",
        " (3, (u'United', u'States', u'of')),\n",
        " (3, (u'US', u'does', u'not')),\n",
        " (3, (u'This', u'is', u'not')),\n",
        " (3, (u'The', u'spirit', u'of')),\n",
        " (3, (u'The', u'majority', u'of')),\n",
        " (3, (u'The', u'United', u'States')),\n",
        " (3, (u'The', u'Middle', u'East')),\n",
        " (3, (u'TO', u'HELL.', u\"It's\")),\n",
        " (3, (u'THEM', u'ALL', u'TO')),\n",
        " (3, (u'SEND', u'THEM', u'ALL')),\n",
        " (3, (u'Iraq', u'and', u'Syria')),\n",
        " (3, (u'In', u'the', u'last')),\n",
        " (3, (u'If', u'you', u'want')),\n",
        " (3, (u'ISIS', u'is', u'a')),\n",
        " (3, (u'I', u'think', u'the')),\n",
        " (3, (u'I', u'do', u'not')),\n",
        " (3, (u'I', u'am', u'not')),\n",
        " (3, (u'How', u'do', u'you')),\n",
        " (3, (u'Country', u'in', u'the')),\n",
        " (3, (u'Air', u'strikes', u'are')),\n",
        " (3, (u'ALL', u'TO', u'HELL.')),\n",
        " (3, (u'15', u'years,', u'most')),\n",
        " (3, (u'(more', u'than', u'half)'))]"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngrammer(raw, 3, threshold = 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 89,
       "text": [
        "[(18, (u'the', u'Middle', u'East')),\n",
        " (18, (u'in', u'the', u'Middle')),\n",
        " (18, (u'carried', u'out', u'by')),\n",
        " (13, (u'out', u'by', u'non-Muslims.')),\n",
        " (13, (u'of', u'terrorist', u'attacks')),\n",
        " (12, (u'the', u'majority', u'of')),\n",
        " (12, (u'majority', u'of', u'terrorist')),\n",
        " (12, (u'around', u'the', u'world'))]"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Concordancing with regular expressions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've already done a bit of concordancing. In discourse-analytic research, concordancing is often used to perform thematic categorisation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.Text(tokens) # formats our tokens for concordancing\n",
      "text.concordance(\"muslims\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Displaying 25 of 60 matches:\n",
        "urselves . I am not saying that all Muslims are like this . However , I am sayi\n",
        " a result of long term migration of Muslims into our Country we now have a burg\n",
        " shut the gate as we now have young Muslims , embracing ISIS ideals and as far \n",
        " of over-sensationalized crap . The Muslims are actually less trouble than are \n",
        "aying the statistics will show that Muslims are more likely to be criminal than\n",
        "ty . What about crimes committed by Muslims whose Grandparents emigrated here ,\n",
        ", would be collateral damage as the Muslims promote their promise to 'have the \n",
        "anda ) Greens -win , I am comparing Muslims to Storm Troopers . Do you have any\n",
        "ers . Do you have any idea what the Muslims are up to in Iraq ? ISIS are commit\n",
        "uld be 'Aussie ' , not Muslim . All Muslims ? You are on a hate speech , hiding\n",
        "iding behind a keyboard , about all muslims and that makes you a fanatical terr\n",
        "nt history ) . The Sunni and Shiite muslims have been killing each other for 12\n",
        "ehalf of Islam , are perpetrated by Muslims . Well it 's YOUR link , not mine .\n",
        "lims . That 's not to say that some Muslims do n't indulge in terrorism : they \n",
        "LAM . It was only 20 years ago that Muslims of a certain region were asking Aus\n",
        " public would have laughed it off . Muslims have a tendency to take their relig\n",
        " the same level of criminality that Muslims do , then rape , murder , and a hea\n",
        " for the bodycount , it appears the muslims are winning that one . How many dea\n",
        "he Azzizzis had lived in peace with Muslims in the Middle East for thousands of\n",
        " He stated that through that period Muslims had not try to convert them by forc\n",
        "he mainstream of 500,000 Australian Muslims do not support this ideology . He a\n",
        "heir soil , they will turn on other Muslims ... Their point is to kill and noth\n",
        "d through . I dont want to hate all muslims ... .but its not easy when they see\n",
        "in the world . And , just to note , Muslims who do n't condemn Muslim terrorist\n",
        "erned about their heads : the crazy Muslims would n't think twice about choppin\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could even our stemmed corpus here:\n",
      "text = nltk.Text(stemmed)\n",
      "text.concordance(\"muslims\")"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You get no matches in the latter case, because all instances of *muslims* were stemmed to *muslim*.\n",
      "\n",
      "A problem with the NLTK concordancer is that it only works with individual tokens.\n",
      "\n",
      "What if we want to find words that end with **ment*, or words beginning with *poli**?\n",
      "\n",
      "We already searched text with Regular Expressions. It's not much more work to build regex functionality into our own concordancer.\n",
      "\n",
      "From running the code below, you can see that bracketting sections of our regex causes results to split into lists:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aussie = r'(?i)(aussie|ozzie|ozzy)'\n",
      "searchpattern = re.compile(r\"(.*)\" + aussie + r\"(.*)\")\n",
      "search = re.findall(searchpattern, raw)\n",
      "pprint.pprint(search[:5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'What a load of over-sensationalized crap. The Muslims are actually less trouble than are the ',\n",
        "  u'Aussie',\n",
        "  u's. And if we had never interfered in their countries they would be no trouble to us at all.'),\n",
        " (u\"So you're saying the statistics will show that Muslims are more likely to be criminal than \",\n",
        "  u'Aussie',\n",
        "  u\"s? And we've been interfering in the Mid East for over one hundred years now, time for us to 'pack' off out of there, don't you think!\"),\n",
        " (u\"That's a useless comparison ....Muslim is a religion, and Australian is a nationality. What about crimes committed by Muslims whose Grandparents emigrated here, do you classify them as Muslim or \",\n",
        "  u'Aussie',\n",
        "  u'???'),\n",
        " (u\"As many as 100,000 Aboriginals may have been slaughtered by 'Aussies', I don't blame Christianity for that. And what about the grandparents, if they've behaved 'worse' it would be because of the influence of their new home. How could it be anything else? They would have had their \",\n",
        "  u'aussie',\n",
        "  u'ness brow-beaten into them.'),\n",
        " (u\"But still, comparing religion and country of birth is a circular argument. Any Australian born Muslim who commits a crime under your definition would be '\",\n",
        "  u'Aussie',\n",
        "  u\"', not Muslim.\")]\n"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well, it's ugly, but it works. We can see five bracketted results, each containing three strings. The first and third strings are the left-context and right-context. The second of the three strings is the search term.\n",
      "\n",
      "These three sections are, with a bit of tweaking, the same as the output given by a concordancer.\n",
      "\n",
      "Let's go ahead and turn our regex seacher into a concordancer:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def concordancer(text, regex):\n",
      "    \"\"\"Concordance using regular expressions\"\"\"\n",
      "    import re\n",
      "    # limit context to 30 characters max\n",
      "    searchpattern = re.compile(r\"(.{,30})(\\b\" + regex + r\"\\b)(.{,30})\")\n",
      "    # find all instances of our regex\n",
      "    search = re.findall(searchpattern, raw)\n",
      "    for result in search:\n",
      "        #join each result with a tab, and print\n",
      "        print(\"\\t\".join(result).expandtabs(20)) \n",
      "        # expand tabs helps align results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "concordancer(raw, r'aus.*?')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "se? They would have had their           aussieness           brow-beaten into them.\n",
        "reat man. You'd go for a good           aussie               man like him. In Pakistan the\n",
        "rives on our doorstep you and           aussie               will be well and truly under \n",
        " gives the go ahead for young           australians          to head into the middle east \n",
        "ir.they have shown loyalty to           aussies              and we should repay the court\n",
        "you were on a train with 1000           aussies              on it and one muslim but that\n",
        "ope. for some reason he hates           aussies              while still living here. Why \n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great! With six lines of code, we've officially created a function that improves on the one provided by NLTK! And think how easy it would be to add more functionality: an argument dictating the size of the window (currently 30 characters), or printing line numbers beside matches, would be pretty easy to add, as well.\n",
      "\n",
      "> Adding too much functionality is known as *feature creep*. It's often best to keep your functions simple and more varied. An old adage in programming is to *make each program do one thing well*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the cells below, try concordancing a few things. Also try creating variables with concordance results, and then manipulate the lists. If you encounter problems with the way the concordancer runs, alter the function and redefine it. If you want, try implementing the window size variable!\n",
      "\n",
      "> **Tip:** If you wanted to get really creative, you could try stemming concordance or n-gram results!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Summary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's the end of session three! Great work.\n",
      "\n",
      "So, some of these tasks are a little dry---seeing results as lists of words and scores isn't always a lot of fun. But ultimately, they're pretty important things to know if you want to avoid the 'black box approach', where you simply dump words into a machine and analyse what the machine spits out.\n",
      "\n",
      "Remember that almost every task in corpus linguistics/distance reading depends on how we segment our data into sentences, clauses, words, etc.\n",
      "\n",
      "Building a stemmer from scratch taught us how to use regular expressions, and their power. But, we also saw that they weren't perfect for the task. In later lessons, we'll use more advanced methods to normalise our data. \n",
      "\n",
      "*See you tomorrow!*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Bibliography"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"firth\"></a>\n",
      "Firth, J. (1957).  *A Synopsis of Linguistic Theory 1930-1955*. In: Studies in Linguistic Analysis, Philological Society, Oxford; reprinted in Palmer, F. (ed.) 1968 Selected Papers of J. R. Firth, Longman, Harlow.\n",
      "\n",
      "<a id=\"ref:hymes\"></a>\n",
      "Hymes, D. (1972). On communicative competence. In J. Pride & J. Holmes (Eds.), Sociolinguistics (pp. 269-293). Harmondsworth: Penguin Books. Retrieved from [http://humanidades.uprrp.edu/smjeg/reserva/Estudios%20Hispanicos/espa3246/Prof%20Sunny%20Cabrera/ESPA%203246%20-%20On%20Communicative%20Competence%20p%2053-73.pdf](http://humanidades.uprrp.edu/smjeg/reserva/Estudios%20Hispanicos/espa3246/Prof%20Sunny%20Cabrera/ESPA%203246%20-%20On%20Communicative%20Competence%20p%2053-73.pdf)\n",
      "\n",
      "<a id=\"ref:widdowson\"></a>\n",
      "Widdowson, H. G. (2000). On the limitations of linguistics applied. Applied Linguistics, 21(1), 3. Available at [http://applij.oxfordjournals.org/content/21/1/3.short](http://applij.oxfordjournals.org/content/21/1/3.short)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}